\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\bibstyle{plain}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\par \penalty \@M \unhbox \voidb@x \hbox {}\hfill {\nag@@warning@vi  \bfseries  Page}\par \penalty \@M }
\citation{Strategy2013}
\citation{Burbidge2007}
\citation{Tibshirani1996}
\citation{blumensath2007difference}
\citation{Tibshirani1996}
\citation{Ji2008}
\citation{Ji2008}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{v}{section*.1}}
\citation{atia2}
\@writefile{lof}{\par \penalty \@M \textbf  {{\scshape  Figure} \hfill Page}\par \penalty \@M }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\citation{Strategy2013}
\citation{Strategy2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{figure.1.1}}
\newlabel{spectrumalloc}{{\M@TitleReference {1.1}{A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{A digram of current Spectral allocation \cite {Strategy2013}}{figure.1.1}{}}
\citation{Burbidge2007}
\citation{Burbidge2007}
\citation{Candes2006}
\citation{donoho2}
\citation{Donoho}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{figure.1.2}}
\newlabel{frequtil}{{\M@TitleReference {1.2}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}{figure.1.2}{}}
\citation{mishali2010theory}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Classical Sensing}{11}{chapter.2}}
\newlabel{chap:classical-sensning}{{\M@TitleReference {2}{Classical Sensing}}{11}{Classical Sensing}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{11}{section.2.1}}
\citation{shannon2001mathematical}
\citation{nyquist2002certain}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Classical Sensing}{12}{section.2.2}}
\newlabel{sec:classic-sensing}{{\M@TitleReference {2.2}{Classical Sensing}}{12}{Classical Sensing}{section.2.2}{}}
\citation{nyquist2002certain}
\newlabel{shah}{{\M@TitleReference {2.2}{Classical Sensing}}{13}{Classical Sensing}{equation.2.9}{}}
\citation{yucek2009survey}
\newlabel{h1}{{\M@TitleReference {2.2}{Classical Sensing}}{15}{Classical Sensing}{equation.2.17}{}}
\newlabel{h2}{{\M@TitleReference {2.2}{Classical Sensing}}{15}{Classical Sensing}{equation.2.18}{}}
\newlabel{likeratio}{{\M@TitleReference {2.2}{Classical Sensing}}{15}{Classical Sensing}{equation.2.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Classical Sensing Techniques}{15}{section.2.3}}
\newlabel{sec:techniques}{{\M@TitleReference {2.3}{Classical Sensing Techniques}}{15}{Classical Sensing Techniques}{section.2.3}{}}
\citation{yucek2009survey}
\citation{yucek2009survey}
\citation{xie2009optimal}
\citation{hamdi2010impact}
\citation{sahai2004some}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Energy Detection}{16}{subsection.2.3.1}}
\newlabel{sec:energy-detection}{{\M@TitleReference {2.3.1}{Energy Detection}}{16}{Energy Detection}{subsection.2.3.1}{}}
\citation{zhang2011adaptive}
\citation{olivieri2005scalable}
\citation{tandra2008snr}
\citation{oude2011lowering}
\citation{ye2007spectrum}
\citation{kim2007cyclostationary}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Cyclostationary Feature Detection}{17}{subsection.2.3.2}}
\citation{Ghozzi2006}
\newlabel{cyclic-covarience}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{18}{Cyclostationary Feature Detection}{equation.2.28}{}}
\newlabel{c1}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{18}{Cyclostationary Feature Detection}{equation.2.32}{}}
\newlabel{c2}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{18}{Cyclostationary Feature Detection}{equation.2.33}{}}
\citation{lunden2007spectrum}
\citation{cabric2004implementation}
\citation{vcabric2005physical}
\citation{Ghozzi2006}
\citation{cabric2004implementation}
\citation{yucek2009survey}
\citation{bhargavi2010performance}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Matched Filtering}{19}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Distributed Approaches to Spectrum Sensing}{19}{subsection.2.3.4}}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{19}|hyperpage}{19}
\citation{oksanen2010characterization}
\citation{cho2015weighted}
\citation{ma2008soft}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@@wrindexm@m{nicethesis}{\glossaryentry{FC?\glossentry{fc}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@@wrindexm@m{nicethesis}{\glossaryentry{CR?\glossentry{cr}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Limitations}{20}{subsection.2.3.5}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Compressive Sensing}{23}{chapter.3}}
\newlabel{chap:cs}{{\M@TitleReference {3}{Compressive Sensing}}{23}{Compressive Sensing}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{23}{section.3.1}}
\newlabel{sec:csinto}{{\M@TitleReference {3.1}{Introduction}}{23}{Introduction}{section.3.1}{}}
\@@wrindexm@m{nicethesis}{\glossaryentry{CS?\glossentry{cs}|setentrycounter[]{page}\glsnumberformat}{23}|hyperpage}{23}
\citation{unser2000sampling,}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Preliminaries}{24}{section.3.2}}
\newlabel{sec:prelims}{{\M@TitleReference {3.2}{Preliminaries}}{24}{Preliminaries}{section.3.2}{}}
\citation{prony1795essai}
\citation{Caratheodory1907}
\citation{claerbout1973robust}
\citation{taylor1979deconvolution}
\citation{Santosa1986}
\citation{candes2008introduction}
\@@wrindexm@m{nicethesis}{\glossaryentry{CS?\glossentry{cs}|setentrycounter[]{page}\glsnumberformat}{25}|hyperpage}{25}
\newlabel{inner-product-repr}{{\M@TitleReference {3.2}{Preliminaries}}{25}{Preliminaries}{equation.3.1}{}}
\newlabel{vector-repr}{{\M@TitleReference {3.2}{Preliminaries}}{26}{Preliminaries}{equation.3.2}{}}
\newlabel{CSequation}{{\M@TitleReference {3.2}{Preliminaries}}{26}{Preliminaries}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}RIP and Stable Embeddings}{26}{subsection.3.2.1}}
\newlabel{sparse-basis-expanson}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{26}{Sparsity}{equation.3.4}{}}
\newlabel{def:alpha}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{26}{Sparsity}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{27}{figure.3.1}}
\newlabel{l1l2}{{\M@TitleReference {3.1}{A visualisation of the Compressive Sensing problem as an under-determined system}}{27}{A visualisation of the Compressive Sensing problem as an under-determined system}{figure.3.1}{}}
\newlabel{cs-model}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{27}{RIP and Stable Embeddings}{equation.3.7}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.2.2}{RIP and Stable Embeddings}}{27}{RIP}{theorem.3.2.2}{}}
\citation{shalev2014understanding}
\newlabel{def:RIP}{{\M@TitleReference {3.2.2}{RIP and Stable Embeddings}}{28}{RIP}{equation.3.8}{}}
\newlabel{def:d-stable}{{\M@TitleReference {3.2.7}{RIP and Stable Embeddings}}{28}{\(\delta \)-stable embedding}{equation.3.10}{}}
\citation{Candes2006}
\citation{davenport2010signal}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Incoherence}{29}{section.3.3}}
\newlabel{minsamples}{{\M@TitleReference {3.3.2}{Incoherence}}{29}{Reconstruction from Compressive measurements \cite {Candes2006}}{equation.3.11}{}}
\citation{baraniuk2008simple}
\citation{baraniuk2008simple}
\citation{baraniuk2008simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Random Matrix Constructions}{30}{subsection.3.3.1}}
\newlabel{sec:mtx-contruction}{{\M@TitleReference {3.3.1}{Random Matrix Constructions}}{30}{Random Matrix Constructions}{subsection.3.3.1}{}}
\newlabel{cond:norm-pres}{{\M@TitleReference {1}{Random Matrix Constructions}}{30}{Norm preservation}{condition.1}{}}
\newlabel{cond:sub-Gauss}{{\M@TitleReference {2}{Random Matrix Constructions}}{30}{sub-Gaussian}{condition.2}{}}
\newlabel{cond:sub-Gauss concetration}{{3.12}{30}{sub-Gaussian}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Wishart Matrices}{30}{subsection.3.3.2}}
\citation{levequeMatrices}
\newlabel{remark: exp AtA}{{\M@TitleReference {3.3.8}{Wishart Matrices}}{31}{}{theorem.3.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reconstruction Objectives}{31}{section.3.4}}
\citation{Chen1998a}
\citation{tibshirani1996regression}
\citation{hoerl1970ridge}
\citation{breiman1995better}
\citation{hastie2005elements}
\citation{Elad2010}
\newlabel{program:bp}{{3.19}{32}{Reconstruction Objectives}{equation.3.19}{}}
\newlabel{program:lasso}{{3.20}{32}{Reconstruction Objectives}{equation.3.20}{}}
\newlabel{program:Ridge-regression}{{3.21}{32}{Reconstruction Objectives}{equation.3.21}{}}
\newlabel{program:ell0}{{3.22}{32}{Reconstruction Objectives}{equation.3.22}{}}
\newlabel{soln:lasso}{{3.23}{32}{Reconstruction Objectives}{equation.3.23}{}}
\newlabel{soln:ridge}{{3.24}{32}{Reconstruction Objectives}{equation.3.24}{}}
\newlabel{soln:l0}{{3.25}{32}{Reconstruction Objectives}{equation.3.25}{}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{zou2005regularization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{33}{figure.3.2}}
\newlabel{fig:l1l2}{{\M@TitleReference {3.2}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{33}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.3.2}{}}
\newlabel{program:enat}{{3.26}{33}{Reconstruction Objectives}{equation.3.26}{}}
\newlabel{program:enat}{{3.27}{33}{Reconstruction Objectives}{equation.3.27}{}}
\citation{candes2007dantzig}
\citation{candes2007dantzig}
\citation{bickel2009simultaneous}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Reconstruction Algorithms}{34}{section.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Convex Algorithms}{34}{subsection.3.5.1}}
\citation{figueiredo2003algorithm}
\citation{wen2015efficient}
\citation{oxvig2012improving}
\citation{mohimani2010sparse}
\citation{tropp2007signal}
\citation{dai2009subspace}
\citation{blumensath2007difference}
\citation{blumensath2007difference}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{35}{figure.3.3}}
\newlabel{alg:IST}{{\M@TitleReference {3.3}{The Iterative Soft Thresholding Algorithm}}{35}{The Iterative Soft Thresholding Algorithm}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Greedy Algorithms}{35}{subsection.3.5.2}}
\citation{goldberger1961stepwise}
\citation{pati1993orthogonal}
\citation{mallat1993matching}
\citation{tropp2007signal}
\citation{wen2013improved}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The OMP recovery algorithm}}{36}{figure.3.4}}
\newlabel{alg:omp}{{\M@TitleReference {3.4}{The OMP recovery algorithm}}{36}{The OMP recovery algorithm}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An illustration of the orthognalisation step of OMP. \cite  {blumensath2007difference}}}{36}{figure.3.5}}
\newlabel{fig:OMP}{{\M@TitleReference {3.5}{An illustration of the orthognalisation step of OMP. \cite  {blumensath2007difference}}}{36}{An illustration of the orthognalisation step of OMP. \cite {blumensath2007difference}}{figure.3.5}{}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The AMP recovery algorithm}}{37}{figure.3.6}}
\newlabel{alg:amp}{{\M@TitleReference {3.6}{The AMP recovery algorithm}}{37}{The AMP recovery algorithm}{figure.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Bayesian Algorithms}{37}{subsection.3.5.3}}
\citation{Baron2010}
\citation{Ji2008}
\citation{Ji2008}
\citation{neal2011mcmc}
\citation{Yedidia2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{38}{figure.3.7}}
\newlabel{laplacenormal}{{\M@TitleReference {3.7}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{38}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.3.7}{}}
\citation{metzler2014denoising}
\citation{Zhang2011b}
\citation{Zhang2011b}
\citation{mishali2010theory}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{39}{figure.3.8}}
\newlabel{fig:bayesiancs}{{\M@TitleReference {3.8}{The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{39}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Compressive Sensing Architectures}{39}{section.3.6}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {3.6}{Compressive Sensing Architectures}}{39}{Compressive Sensing Architectures}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Modulated Wideband Converter}{39}{subsection.3.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{40}{figure.3.9}}
\newlabel{msevssnr0}{{\M@TitleReference {3.9}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{40}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.3.9}{}}
\citation{kirolos2006analog}
\citation{harms2013constrained}
\newlabel{system}{{3.39}{41}{Modulated Wideband Converter}{equation.3.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Random Demodulator}{41}{subsection.3.6.2}}
\citation{yoo2012design}
\citation{Ji2008}
\citation{Ji2008}
\citation{massoud2011efficient}
\citation{harms2013constrained}
\citation{Slavinsky2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{43}{figure.3.10}}
\newlabel{bayesiancs}{{\M@TitleReference {3.10}{The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{43}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Compressive Multiplexer}{43}{subsection.3.6.3}}
\@@wrindexm@m{nicethesis}{\glossaryentry{CMUX?\glossentry{cmux}|setentrycounter[]{page}\glsnumberformat}{43}|hyperpage}{43}
\citation{davenport2012pros}
\@@wrindexm@m{nicethesis}{\glossaryentry{RD?\glossentry{rd}|setentrycounter[]{page}\glsnumberformat}{43}|hyperpage}{44}
\@@wrindexm@m{nicethesis}{\glossaryentry{MWC?\glossentry{mwc}|setentrycounter[]{page}\glsnumberformat}{43}|hyperpage}{44}
\@@wrindexm@m{nicethesis}{\glossaryentry{CMUX?\glossentry{cmux}|setentrycounter[]{page}\glsnumberformat}{44}|hyperpage}{44}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Compressive Sampling for Spectrum Sensing}{44}{section.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Noise Folding}{44}{subsection.3.7.1}}
\citation{davenport2012pros}
\citation{davenport2012pros}
\citation{Tian2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Dynamic Range}{45}{subsection.3.7.2}}
\citation{Tian2007}
\citation{polo2009compressive}
\citation{Tian2007}
\citation{sundman2010use}
\citation{tian2012cyclic}
\citation{tian2012cyclic}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}CS models}{46}{subsection.3.7.3}}
\citation{romero2016compressive}
\citation{havary2010compressive}
\citation{braun2009signal}
\citation{Davenport2007}
\citation{davenport2010signal}
\citation{Davenport2007}
\citation{lexa2011compressive}
\citation{Duarte}
\citation{Duarte}
\citation{Ma2014b}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Mishali2010}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Sundman2013a}
\citation{schizas2008consensus}
\citation{schizas2008consensus2}
\citation{verlant2012multiband}
\citation{bodart2015multiband}
\citation{Davenport2010}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}ADMM}{49}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{49}{section.4.1}}
\newlabel{chap:admm}{{\M@TitleReference {4.1}{Introduction}}{49}{Introduction}{section.4.1}{}}
\citation{mackay2003information}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background}{50}{section.4.2}}
\newlabel{sec:admm}{{\M@TitleReference {4.2}{Background}}{50}{Background}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Preliminary Definitiona}{50}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Lagrangian Formulation}{51}{subsection.4.2.2}}
\citation{Boyd2010a}
\citation{parikh2014proximal}
\citation{rockafellar1976monotone}
\citation{douglas1956numerical}
\citation{eckstein1992douglas}
\citation{Bristow2014}
\citation{heredia2015consensus}
\citation{sawatzky2014proximal}
\citation{o2013splitting}
\citation{Shi2013}
\citation{nishihara2015general}
\newlabel{dual-ascent}{{4.24}{53}{Dual Ascent}{equation.4.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}ADMM}{53}{subsection.4.2.3}}
\citation{ghadimi2015optimal}
\citation{goldstein2014fast}
\citation{chen2016direct}
\citation{nesterov2005smooth}
\newlabel{admm}{{4.25}{54}{ADMM}{equation.4.25}{}}
\newlabel{admm_aug_lagrangian}{{4.26}{54}{ADMM}{equation.4.26}{}}
\newlabel{admm_algo}{{4.28}{54}{ADMM}{equation.4.28}{}}
\newlabel{minl2_aug_lagrangian}{{4.34}{55}{Minimum norm solution of a Linear Equation}{equation.4.34}{}}
\newlabel{minl2_algo}{{4.36}{55}{Minimum norm solution of a Linear Equation}{equation.4.36}{}}
\newlabel{LASSO}{{4.38}{55}{LASSO}{equation.4.38}{}}
\citation{petersen2008matrix}
\newlabel{LASSO-L0}{{4.39}{56}{LASSO}{equation.4.39}{}}
\newlabel{admm_algo_lasso}{{4.42}{56}{LASSO}{equation.4.42}{}}
\newlabel{dellx}{{4.43}{56}{LASSO}{equation.4.43}{}}
\newlabel{optx}{{4.45}{56}{LASSO}{equation.4.45}{}}
\newlabel{dellz-positive}{{4.46}{57}{LASSO}{equation.4.46}{}}
\newlabel{dellz-negative}{{4.47}{57}{LASSO}{equation.4.47}{}}
\newlabel{zbounds}{{4.48}{57}{LASSO}{equation.4.48}{}}
\newlabel{optz}{{4.49}{57}{LASSO}{equation.4.49}{}}
\citation{moreau1965proximite}
\newlabel{hatx}{{4.52}{58}{}{equation.4.52}{}}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Proximity Operator}{59}{section.4.3}}
\newlabel{sec:prox}{{\M@TitleReference {4.3}{The Proximity Operator}}{59}{The Proximity Operator}{section.4.3}{}}
\newlabel{def:prox_operator}{{\M@TitleReference {4.3.1}{The Proximity Operator}}{59}{Proximity Operator}{theorem.4.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Properties}{59}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{60}{section*.3}}
\@writefile{toc}{\contentsline {subsubsection}{Examples}{61}{section*.4}}
\newlabel{consensus}{{4.3.12}{62}{Consensus}{theorem.4.3.12}{}}
\newlabel{admm_consensus}{{4.3.12}{62}{Consensus}{theorem.4.3.12}{}}
\newlabel{consensus_iterations}{{4.85}{62}{Consensus}{equation.4.85}{}}
\newlabel{simple_consensus_iterations}{{4.87}{62}{Consensus}{equation.4.87}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Acceleration}{63}{section.4.4}}
\newlabel{sec:accel}{{\M@TitleReference {4.4}{Acceleration}}{63}{Acceleration}{section.4.4}{}}
\citation{goldstein2014fast}
\citation{su2014differential}
\citation{Zhang2011b}
\citation{mota2013d}
\citation{mota2013d}
\citation{mota2013d}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Optimisation on Graphs}{65}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{65}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Constrained Optimisation on Graphs}{66}{section.5.2}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {5.2}{Constrained Optimisation on Graphs}}{66}{Constrained Optimisation on Graphs}{section.5.2}{}}
\newlabel{eq:system}{{\M@TitleReference {5.2}{Constrained Optimisation on Graphs}}{66}{Constrained Optimisation on Graphs}{equation.5.1}{}}
\newlabel{constrainedbp}{{5.3}{66}{Constrained Optimisation on Graphs}{equation.5.3}{}}
\citation{Boyd2010a}
\newlabel{constrainedbp}{{5.4}{67}{Constrained Optimisation on Graphs}{equation.5.4}{}}
\newlabel{barxc}{{5.5}{67}{}{equation.5.5}{}}
\newlabel{compact-constraints}{{5.6}{67}{Constrained Optimisation on Graphs}{equation.5.6}{}}
\newlabel{constrainedbp1}{{5.7}{67}{Constrained Optimisation on Graphs}{equation.5.7}{}}
\citation{Boyd2010a}
\newlabel{aug-lagrange}{{5.8}{68}{Constrained Optimisation on Graphs}{equation.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of a network}}{68}{figure.5.1}}
\newlabel{efig:ex-network}{{\M@TitleReference {5.1}{An example of a network}}{68}{An example of a network}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{69}{figure.5.2}}
\newlabel{fig:incidence-matrix}{{\M@TitleReference {5.2}{The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{69}{The incidence matrix associated with Figure \eqref {efig:ex-network}}{figure.5.2}{}}
\citation{cotter2005sparse}
\newlabel{generic-iterations}{{5.12}{70}{Constrained Optimisation on Graphs}{equation.5.12}{}}
\newlabel{dadmm_algo_lasso}{{5.17}{70}{}{equation.5.17}{}}
\citation{bazerque2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}DADMM-Lasso}{71}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}DADMM-Dantzig-Selector}{71}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}DADMM-MMV}{71}{subsection.5.2.3}}
\@@wrindexm@m{nicethesis}{\glossaryentry{MMV?\glossentry{mmv}|setentrycounter[]{page}\glsnumberformat}{70}|hyperpage}{71}
\@@wrindexm@m{nicethesis}{\glossaryentry{MMV?\glossentry{mmv}|setentrycounter[]{page}\glsnumberformat}{71}|hyperpage}{71}
\newlabel{program:bpmmv}{{5.19}{71}{DADMM-MMV}{equation.5.19}{}}
\newlabel{program:lassommv}{{5.20}{71}{DADMM-MMV}{equation.5.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{71}{section.5.3}}
\newlabel{sec:results}{{\M@TitleReference {5.3}{Results}}{71}{Results}{section.5.3}{}}
\citation{goldstein2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{72}{figure.5.3}}
\newlabel{msevssnr0}{{\M@TitleReference {5.3}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{72}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Conclusions}{72}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{73}{figure.5.4}}
\newlabel{msevssnr1}{{\M@TitleReference {5.4}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{73}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}{figure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{73}{figure.5.5}}
\newlabel{fig:differentLambda}{{\M@TitleReference {5.5}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{73}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{74}{figure.5.6}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.6}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{74}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{74}{figure.5.7}}
\newlabel{fig:spline_recon}{{\M@TitleReference {5.7}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{74}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{75}{figure.5.8}}
\newlabel{fig:steps_wavelets}{{\M@TitleReference {5.8}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{75}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{75}{figure.5.9}}
\newlabel{fig:wavelet_recon}{{\M@TitleReference {5.9}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{75}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{figure.5.10}}
\newlabel{fig:wavelet_recon_no_pwer_2}{{\M@TitleReference {5.10}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{figure.5.11}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.11}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{77}{figure.5.12}}
\newlabel{fig:steps_difference}{{\M@TitleReference {5.12}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{77}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{77}{figure.5.13}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.13}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{77}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The progress of DADMM (blue) vs DBP (red) solvers as a function of the number of iterations. }}{78}{figure.5.14}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.14}{The progress of DADMM (blue) vs DBP (red) solvers as a function of the number of iterations. }}{78}{The progress of DADMM (blue) vs DBP (red) solvers as a function of the number of iterations. }{figure.5.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces The solutions produced by DADMM and DBP}}{78}{figure.5.15}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.15}{The solutions produced by DADMM and DBP}}{78}{The solutions produced by DADMM and DBP}{figure.5.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces The progress of a DADMM (blue) and DBP (red) }}{79}{figure.5.16}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.16}{The progress of a DADMM (blue) and DBP (red) }}{79}{The progress of a DADMM (blue) and DBP (red) }{figure.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{79}{figure.5.17}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.17}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{79}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{80}{figure.5.18}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.18}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{80}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{80}{figure.5.19}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.19}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{80}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.19}{}}
\citation{akan2009cognitive}
\citation{Candes2006}
\citation{mishali2010theory}
\citation{polo2009compressive}
\citation{tropp2010beyond}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Sensing with Heavyside Basis}{81}{chapter.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{81}{section.6.1}}
\citation{Zhang2011b}
\citation{tian2006wavelet}
\citation{tian2006wavelet}
\citation{ling2015dlm}
\citation{mokhtari2015dqm}
\citation{mota2013d}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Signal Model}{82}{section.6.2}}
\newlabel{basis}{{6.1}{83}{Signal Model}{equation.6.1}{}}
\newlabel{basis-expansion}{{6.4}{83}{Signal Model}{equation.6.4}{}}
\newlabel{def:a}{{\M@TitleReference {6.2.1}{Signal Model}}{83}{}{equation.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sensing Model}{84}{section.6.3}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {6.3}{Sensing Model}}{84}{Sensing Model}{section.6.3}{}}
\newlabel{dist_system}{{6.9}{84}{Sensing Model}{equation.6.9}{}}
\newlabel{system}{{6.10}{84}{Sensing Model}{equation.6.10}{}}
\newlabel{opt}{{6.11}{84}{Sensing Model}{equation.6.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Constrained Optimisation on Graphs}{84}{section.6.4}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {6.4}{Constrained Optimisation on Graphs}}{84}{Constrained Optimisation on Graphs}{section.6.4}{}}
\newlabel{barxc}{{6.13}{85}{}{equation.6.13}{}}
\newlabel{constrainedbp}{{6.14}{85}{Constrained Optimisation on Graphs}{equation.6.14}{}}
\newlabel{compact-constraints}{{6.15}{85}{Constrained Optimisation on Graphs}{equation.6.15}{}}
\newlabel{constrainedbp1}{{6.16}{85}{Constrained Optimisation on Graphs}{equation.6.16}{}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{aug-lagrange}{{6.17}{86}{Constrained Optimisation on Graphs}{equation.6.17}{}}
\newlabel{generic-iterations}{{6.21}{87}{Constrained Optimisation on Graphs}{equation.6.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The algorithm at Node \(j\)}}{88}{figure.6.1}}
\newlabel{DADMM}{{\M@TitleReference {6.1}{The algorithm at Node \(j\)}}{88}{The algorithm at Node \(j\)}{figure.6.1}{}}
\newlabel{dadmm_algo_lasso}{{6.26}{88}{Constrained Optimisation on Graphs}{equation.6.26}{}}
\citation{Chen1998}
\citation{bazerque2008}
\citation{bazerque2008}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{89}{section.6.5}}
\newlabel{sec:results}{{\M@TitleReference {6.5}{Results}}{89}{Results}{section.6.5}{}}
\citation{shi2014linear}
\citation{nishihara2015general}
\citation{su2014differential}
\citation{mokhtari2015dqm}
\citation{ling2015dlm}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{90}{figure.6.2}}
\newlabel{different_sigs}{{\M@TitleReference {6.2}{Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{90}{Left to right: (a) The original signal. (b) The gradient \eqref {def:a} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusions}{90}{section.6.6}}
\citation{goldstein2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{91}{figure.6.3}}
\newlabel{msevssnr0}{{\M@TitleReference {6.3}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{91}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{92}{figure.6.4}}
\newlabel{fig:differentLambda}{{\M@TitleReference {6.4}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{92}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations}{figure.6.4}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Compressive Inference}{93}{chapter.7}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{93}{section.7.1}}
\citation{Candes2006}
\citation{Donoho2006}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Theory}{94}{section.7.2}}
\citation{Candes2006}
\citation{donoho2004neighborly}
\citation{davenport2010signal}
\citation{davenport2007smashed}
\citation{schnelle2012compressive}
\citation{davenport2010wideband}
\citation{eftekhari2013matched}
\newlabel{minsamples}{{\M@TitleReference {7.2}{Theory}}{95}{Theory}{equation.7.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Compressive Estimation}{96}{section.7.3}}
\newlabel{sec:estimation}{{\M@TitleReference {7.3}{Compressive Estimation}}{96}{Compressive Estimation}{section.7.3}{}}
\newlabel{log-like}{{7.12}{96}{Compressive Estimation}{equation.7.12}{}}
\newlabel{approx-log-like}{{7.16}{97}{Compressive Estimation}{equation.7.16}{}}
\newlabel{eq: compressive-estimator}{{7.17}{97}{Compressive Estimation}{equation.7.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Example: Single Spike}{98}{subsection.7.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Estimating a single rectangle}{98}{subsection.7.3.2}}
\newlabel{basis}{{7.23}{98}{Estimating a single rectangle}{equation.7.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces }}{99}{figure.7.1}}
\newlabel{fig:new_basis_25}{{\M@TitleReference {7.1}{}}{99}{}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces }}{99}{figure.7.2}}
\newlabel{fig:rectangle}{{\M@TitleReference {7.2}{}}{99}{}{figure.7.2}{}}
\newlabel{basis-expansion}{{7.24}{99}{Estimating a single rectangle}{equation.7.24}{}}
\newlabel{ss-estimator}{{7.32}{100}{Estimating a single rectangle}{equation.7.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Estimating Frequency spectra}{100}{subsection.7.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces }}{101}{figure.7.3}}
\newlabel{fig:hhat}{{\M@TitleReference {7.3}{}}{101}{}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces ROC for synthetic data, midly noisy}}{101}{figure.7.4}}
\newlabel{fig:hvb}{{\M@TitleReference {7.4}{ROC for synthetic data, midly noisy}}{101}{ROC for synthetic data, midly noisy}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces ROC for synthetic data, very noisy}}{102}{figure.7.5}}
\newlabel{fig:hvb}{{\M@TitleReference {7.5}{ROC for synthetic data, very noisy}}{102}{ROC for synthetic data, very noisy}{figure.7.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Results on OFCOM Data}{103}{chapter.8}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{103}{section.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Data Set}{103}{section.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Results: Distributed Estimation with Heaviside Basis}{103}{section.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Compressive Estimation}{103}{section.8.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{104}{figure.8.1}}
\newlabel{fig:hvb}{{\M@TitleReference {8.1}{Example of classification with OFCOM data, 35 changepoints}}{104}{Example of classification with OFCOM data, 35 changepoints}{figure.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{104}{figure.8.2}}
\newlabel{fig:hvb}{{\M@TitleReference {8.2}{Example of classification with OFCOM data, 55 changepoints}}{104}{Example of classification with OFCOM data, 55 changepoints}{figure.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{105}{figure.8.3}}
\newlabel{fig:hvb}{{\M@TitleReference {8.3}{Example of classification with OFCOM data, 35 changepoints}}{105}{Example of classification with OFCOM data, 35 changepoints}{figure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{105}{figure.8.4}}
\newlabel{fig:hvb}{{\M@TitleReference {8.4}{Example of classification with OFCOM data, 55 changepoints}}{105}{Example of classification with OFCOM data, 55 changepoints}{figure.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{106}{figure.8.5}}
\newlabel{fig:hvb}{{\M@TitleReference {8.5}{Example of classification with OFCOM data, 35 changepoints}}{106}{Example of classification with OFCOM data, 35 changepoints}{figure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{106}{figure.8.6}}
\newlabel{fig:hvb}{{\M@TitleReference {8.6}{Example of classification with OFCOM data, 55 changepoints}}{106}{Example of classification with OFCOM data, 55 changepoints}{figure.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{107}{figure.8.7}}
\newlabel{fig:hvb}{{\M@TitleReference {8.7}{Example of classification with OFCOM data, 35 changepoints}}{107}{Example of classification with OFCOM data, 35 changepoints}{figure.8.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{107}{figure.8.8}}
\newlabel{fig:hvb}{{\M@TitleReference {8.8}{Example of classification with OFCOM data, 55 changepoints}}{107}{Example of classification with OFCOM data, 55 changepoints}{figure.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{108}{figure.8.9}}
\newlabel{fig:hvb}{{\M@TitleReference {8.9}{Example of classification with OFCOM data, 35 changepoints}}{108}{Example of classification with OFCOM data, 35 changepoints}{figure.8.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{108}{figure.8.10}}
\newlabel{fig:hvb}{{\M@TitleReference {8.10}{Example of classification with OFCOM data, 55 changepoints}}{108}{Example of classification with OFCOM data, 55 changepoints}{figure.8.10}{}}
\citation{li5}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {9}Group Testing}{109}{chapter.9}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction and notation}{109}{section.9.1}}
\citation{Dorfman1943}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Group Testing}{110}{subsection.9.1.1}}
\citation{atia2}
\citation{atia2}
\citation{du}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{111}{figure.9.1}}
\newlabel{bayesiancs}{{\M@TitleReference {9.1}{The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{111}{The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}{figure.9.1}{}}
\citation{Hwang1972}
\citation{Aldridge2013}
\citation{Chan2011}
\citation{Baldassini2013}
\newlabel{hwangbound}{{\M@TitleReference {9.1.1}{Bounds}}{113}{Bounds}{equation.9.9}{}}
\newlabel{compbound}{{\M@TitleReference {9.1.1}{Bounds}}{113}{Bounds}{equation.9.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Compressive Sensing}{113}{section*.8}}
\citation{Emma}
\citation{Sejdinovic2010}
\citation{Wadayama}
\citation{Baldassini2013}
\citation{dorfman}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}The Probabilistic group testing problem}{114}{subsection.9.1.2}}
\citation{dorfman}
\citation{du,malyutov}
\citation{malyutov}
\citation{atia,johnsonc8,johnson33}
\citation{Wadayama}
\citation{li5}
\citation{atia2}
\citation{dorfman}
\citation{shental}
\citation{atia,johnsonc10,johnson33,tan}
\citation{atia,tan}
\citation{johnsonc10}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Group testing capacity}{115}{subsection.9.1.3}}
\newlabel{def:capacity}{{\M@TitleReference {9.1.1}{Group testing capacity}}{115}{}{theorem.9.1.1}{}}
\newlabel{eq:lower}{{9.11}{115}{}{equation.9.11}{}}
\citation{johnsonc10}
\citation{li5}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{hwang}
\citation{du}
\citation{johnsonc10}
\citation{}
\citation{li5}
\citation{li5}
\citation{hwang}
\newlabel{eq:upper}{{9.12}{116}{}{equation.9.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Main results}{116}{subsection.9.1.4}}
\newlabel{thm:mainold}{{\M@TitleReference {9.1.4}{Main results}}{116}{\cite {johnsonc10}}{theorem.9.1.4}{}}
\newlabel{eq:bja}{{9.13}{116}{Main results}{equation.9.13}{}}
\newlabel{cor:main}{{\M@TitleReference {9.1.5}{Main results}}{116}{}{theorem.9.1.5}{}}
\citation{li5}
\citation{johnsonc10}
\citation{li5}
\citation{li5}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Algorithms and existing results}{117}{section.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Upper bounds on success probability}{117}{subsection.9.2.1}}
\newlabel{sec:ub}{{\M@TitleReference {9.2.1}{Upper bounds on success probability}}{117}{Upper bounds on success probability}{subsection.9.2.1}{}}
\newlabel{thm:upper}{{\M@TitleReference {9.2.1}{Upper bounds on success probability}}{117}{}{theorem.9.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Binary search algorithms}{117}{subsection.9.2.2}}
\citation{li5}
\citation{li5}
\citation{li5}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{118}{figure.9.2}}
\newlabel{thm:lower}{{\M@TitleReference {9.2.3}{Binary search algorithms}}{118}{}{theorem.9.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Summary of our contribution}{118}{subsection.9.2.3}}
\newlabel{sec:algo}{{\M@TitleReference {9.2.3}{Summary of our contribution}}{118}{Summary of our contribution}{subsection.9.2.3}{}}
\citation{aksoylar,tan}
\citation{Candes2006,donoho2}
\citation{Candes2006,donoho2}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Wider context: sparse inference problems}{119}{subsection.9.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Analysis and new bounds}{119}{section.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Searching a set of bounded ratio}{119}{subsection.9.3.1}}
\newlabel{sec:boundedratio}{{\M@TitleReference {9.3.1}{Searching a set of bounded ratio}}{119}{Searching a set of bounded ratio}{subsection.9.3.1}{}}
\newlabel{cond:ratio}{{\M@TitleReference {3}{Searching a set of bounded ratio}}{119}{Bounded Ratio Condition}{condition.3}{}}
\newlabel{eq:ratio}{{9.14}{119}{Bounded Ratio Condition}{equation.9.14}{}}
\newlabel{lem:sfstep}{{\M@TitleReference {9.3.1}{Searching a set of bounded ratio}}{119}{}{theorem.9.3.1}{}}
\newlabel{eq:depth}{{9.15}{120}{}{equation.9.15}{}}
\newlabel{eq:setbd}{{9.16}{120}{Searching a set of bounded ratio}{equation.9.16}{}}
\newlabel{eq:lengthbd}{{\M@TitleReference {9.17}{Searching a set of bounded ratio}}{120}{Searching a set of bounded ratio}{equation.9.17}{}}
\newlabel{rem:algo}{{\M@TitleReference {9.3.2}{Searching a set of bounded ratio}}{120}{}{theorem.9.3.2}{}}
\newlabel{lem:expset}{{\M@TitleReference {9.3.3}{Searching a set of bounded ratio}}{120}{}{theorem.9.3.3}{}}
\newlabel{eq:tbds}{{9.18}{120}{}{equation.9.18}{}}
\citation{li5}
\citation{li5}
\newlabel{eq:testsperdef}{{\M@TitleReference {9.19}{Searching a set of bounded ratio}}{121}{Searching a set of bounded ratio}{equation.9.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Discarding low probability items}{121}{subsection.9.3.2}}
\newlabel{sec:discard}{{\M@TitleReference {9.3.2}{Discarding low probability items}}{121}{Discarding low probability items}{subsection.9.3.2}{}}
\newlabel{lem:thresh}{{\M@TitleReference {9.3.4}{Discarding low probability items}}{121}{}{theorem.9.3.4}{}}
\newlabel{eq:thetadef}{{9.20}{121}{}{equation.9.20}{}}
\newlabel{eq:setpstar}{{9.21}{121}{}{equation.9.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Searching the entire set}{122}{subsection.9.3.3}}
\newlabel{eq:bincount}{{9.22}{122}{Searching the entire set}{equation.9.22}{}}
\newlabel{def:full}{{\M@TitleReference {9.3.5}{Searching the entire set}}{122}{}{theorem.9.3.5}{}}
\newlabel{prop:splitting}{{\M@TitleReference {9.3.6}{Searching the entire set}}{122}{}{theorem.9.3.6}{}}
\newlabel{it:count}{{\M@TitleReference {1}{Searching the entire set}}{122}{Searching the entire set}{Item.18}{}}
\citation{petrov}
\newlabel{eq:counting}{{9.23}{123}{Searching the entire set}{equation.9.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Bounding the expected number of tests}{123}{subsection.9.3.4}}
\newlabel{sec:expectation}{{\M@TitleReference {9.3.4}{Bounding the expected number of tests}}{123}{Bounding the expected number of tests}{subsection.9.3.4}{}}
\newlabel{prop:overall}{{\M@TitleReference {9.3.7}{Bounding the expected number of tests}}{123}{}{theorem.9.3.7}{}}
\newlabel{eq:tbd}{{9.24}{123}{}{equation.9.24}{}}
\newlabel{eq:total}{{\M@TitleReference {9.25}{Bounding the expected number of tests}}{123}{Bounding the expected number of tests}{equation.9.25}{}}
\newlabel{eq:toopt}{{\M@TitleReference {9.25}{Bounding the expected number of tests}}{123}{Bounding the expected number of tests}{equation.9.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Controlling the error probabilities}{124}{subsection.9.3.5}}
\newlabel{sec:main}{{\M@TitleReference {9.3.5}{Controlling the error probabilities}}{124}{Controlling the error probabilities}{subsection.9.3.5}{}}
\newlabel{thm:bernstein}{{\M@TitleReference {9.3.8}{Controlling the error probabilities}}{124}{Bernstein}{theorem.9.3.8}{}}
\newlabel{eq:bernstein}{{9.26}{124}{Bernstein}{equation.9.26}{}}
\newlabel{thm:main}{{\M@TitleReference {9.3.9}{Controlling the error probabilities}}{124}{}{theorem.9.3.9}{}}
\newlabel{eq:tnec}{{9.27}{124}{}{equation.9.27}{}}
\newlabel{it:part1}{{\M@TitleReference {1}{Controlling the error probabilities}}{124}{}{Item.21}{}}
\newlabel{eq:errorprob}{{9.28}{124}{}{equation.9.28}{}}
\newlabel{it:part2}{{\M@TitleReference {2}{Controlling the error probabilities}}{124}{}{Item.22}{}}
\newlabel{eq:vbd}{{9.29}{124}{Controlling the error probabilities}{equation.9.29}{}}
\newlabel{eq:ratio2}{{\M@TitleReference {9.30}{Controlling the error probabilities}}{125}{Controlling the error probabilities}{equation.9.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Results}{125}{section.9.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{126}{figure.9.3}}
\newlabel{ubvslb}{{\M@TitleReference {9.3}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{126}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}{figure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{126}{figure.9.4}}
\newlabel{testsvsalpha}{{\M@TitleReference {9.4}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{126}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }{figure.9.4}{}}
\citation{li5}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{127}{figure.9.5}}
\newlabel{testsvstheta}{{\M@TitleReference {9.5}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{127}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}{figure.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Discussion}{127}{section.9.5}}
\bibdata{thesis2}
\bibcite{akan2009cognitive}{{1}{}{{}}{{}}}
\bibcite{aksoylar}{{2}{}{{}}{{}}}
\bibcite{johnson33}{{3}{}{{}}{{}}}
\bibcite{atia2}{{4}{}{{}}{{}}}
\bibcite{atia}{{5}{}{{}}{{}}}
\bibcite{johnsonc10}{{6}{}{{}}{{}}}
\bibcite{baraniuk2008simple}{{7}{}{{}}{{}}}
\bibcite{Baron2010}{{8}{}{{}}{{}}}
\bibcite{bazerque2008}{{9}{}{{}}{{}}}
\bibcite{bhargavi2010performance}{{10}{}{{}}{{}}}
\bibcite{bickel2009simultaneous}{{11}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{131}{section*.10}}
\bibcite{blumensath2007difference}{{12}{}{{}}{{}}}
\bibcite{bodart2015multiband}{{13}{}{{}}{{}}}
\bibcite{Boyd2010a}{{14}{}{{}}{{}}}
\bibcite{braun2009signal}{{15}{}{{}}{{}}}
\bibcite{breiman1995better}{{16}{}{{}}{{}}}
\bibcite{Bristow2014}{{17}{}{{}}{{}}}
\bibcite{Burbidge2007}{{18}{}{{}}{{}}}
\bibcite{vcabric2005physical}{{19}{}{{}}{{}}}
\bibcite{cabric2004implementation}{{20}{}{{}}{{}}}
\bibcite{Emma}{{21}{}{{}}{{}}}
\bibcite{candes2007dantzig}{{22}{}{{}}{{}}}
\bibcite{Candes2006}{{23}{}{{}}{{}}}
\bibcite{candes2008introduction}{{24}{}{{}}{{}}}
\bibcite{Caratheodory1907}{{25}{}{{}}{{}}}
\bibcite{Chan2011}{{26}{}{{}}{{}}}
\bibcite{chen2016direct}{{27}{}{{}}{{}}}
\bibcite{Chen1998a}{{28}{}{{}}{{}}}
\bibcite{Chen1998}{{29}{}{{}}{{}}}
\bibcite{cho2015weighted}{{30}{}{{}}{{}}}
\bibcite{claerbout1973robust}{{31}{}{{}}{{}}}
\bibcite{cotter2005sparse}{{32}{}{{}}{{}}}
\bibcite{dai2009subspace}{{33}{}{{}}{{}}}
\bibcite{Davenport2010}{{34}{}{{}}{{}}}
\bibcite{davenport2010signal}{{35}{}{{}}{{}}}
\bibcite{davenport2007smashed}{{36}{}{{}}{{}}}
\bibcite{Davenport2007}{{37}{}{{}}{{}}}
\bibcite{davenport2012pros}{{38}{}{{}}{{}}}
\bibcite{davenport2010wideband}{{39}{}{{}}{{}}}
\bibcite{prony1795essai}{{40}{}{{}}{{}}}
\bibcite{donoho2004neighborly}{{41}{}{{}}{{}}}
\bibcite{donoho2}{{42}{}{{}}{{}}}
\bibcite{Donoho}{{43}{}{{}}{{}}}
\bibcite{Dorfman1943}{{44}{}{{}}{{}}}
\bibcite{douglas1956numerical}{{45}{}{{}}{{}}}
\bibcite{du}{{46}{}{{}}{{}}}
\bibcite{Duarte}{{47}{}{{}}{{}}}
\bibcite{eckstein1992douglas}{{48}{}{{}}{{}}}
\bibcite{eftekhari2013matched}{{49}{}{{}}{{}}}
\bibcite{Elad2010}{{50}{}{{}}{{}}}
\bibcite{figueiredo2003algorithm}{{51}{}{{}}{{}}}
\bibcite{ghadimi2015optimal}{{52}{}{{}}{{}}}
\bibcite{Ghozzi2006}{{53}{}{{}}{{}}}
\bibcite{Tian2007}{{54}{}{{}}{{}}}
\bibcite{goldberger1961stepwise}{{55}{}{{}}{{}}}
\bibcite{goldstein2014fast}{{56}{}{{}}{{}}}
\bibcite{hamdi2010impact}{{57}{}{{}}{{}}}
\bibcite{harms2013constrained}{{58}{}{{}}{{}}}
\bibcite{hastie2005elements}{{59}{}{{}}{{}}}
\bibcite{havary2010compressive}{{60}{}{{}}{{}}}
\bibcite{heredia2015consensus}{{61}{}{{}}{{}}}
\bibcite{hoerl1970ridge}{{62}{}{{}}{{}}}
\bibcite{hwang}{{63}{}{{}}{{}}}
\bibcite{Ji2008}{{64}{}{{}}{{}}}
\bibcite{kim2007cyclostationary}{{65}{}{{}}{{}}}
\bibcite{kirolos2006analog}{{66}{}{{}}{{}}}
\bibcite{levequeMatrices}{{67}{}{{}}{{}}}
\bibcite{lexa2011compressive}{{68}{}{{}}{{}}}
\bibcite{li5}{{69}{}{{}}{{}}}
\bibcite{ling2015dlm}{{70}{}{{}}{{}}}
\bibcite{lunden2007spectrum}{{71}{}{{}}{{}}}
\bibcite{}{{72}{}{{}}{{}}}
\bibcite{Ma2014b}{{73}{}{{}}{{}}}
\bibcite{mackay2003information}{{74}{}{{}}{{}}}
\bibcite{mallat1993matching}{{75}{}{{}}{{}}}
\bibcite{malyutov}{{76}{}{{}}{{}}}
\bibcite{massoud2011efficient}{{77}{}{{}}{{}}}
\bibcite{metzler2014denoising}{{78}{}{{}}{{}}}
\bibcite{Mishali2010}{{79}{}{{}}{{}}}
\bibcite{mishali2010theory}{{80}{}{{}}{{}}}
\bibcite{mohimani2010sparse}{{81}{}{{}}{{}}}
\bibcite{mokhtari2015dqm}{{82}{}{{}}{{}}}
\bibcite{moreau1965proximite}{{83}{}{{}}{{}}}
\bibcite{mota2013d}{{84}{}{{}}{{}}}
\bibcite{neal2011mcmc}{{85}{}{{}}{{}}}
\bibcite{nesterov2005smooth}{{86}{}{{}}{{}}}
\bibcite{nishihara2015general}{{87}{}{{}}{{}}}
\bibcite{nyquist2002certain}{{88}{}{{}}{{}}}
\bibcite{o2013splitting}{{89}{}{{}}{{}}}
\bibcite{oksanen2010characterization}{{90}{}{{}}{{}}}
\bibcite{olivieri2005scalable}{{91}{}{{}}{{}}}
\bibcite{oude2011lowering}{{92}{}{{}}{{}}}
\bibcite{oxvig2012improving}{{93}{}{{}}{{}}}
\bibcite{parikh2014proximal}{{94}{}{{}}{{}}}
\bibcite{pati1993orthogonal}{{95}{}{{}}{{}}}
\bibcite{petersen2008matrix}{{96}{}{{}}{{}}}
\bibcite{petrov}{{97}{}{{}}{{}}}
\bibcite{polo2009compressive}{{98}{}{{}}{{}}}
\bibcite{rockafellar1976monotone}{{99}{}{{}}{{}}}
\bibcite{romero2016compressive}{{100}{}{{}}{{}}}
\bibcite{sahai2004some}{{101}{}{{}}{{}}}
\bibcite{Santosa1986}{{102}{}{{}}{{}}}
\bibcite{sawatzky2014proximal}{{103}{}{{}}{{}}}
\bibcite{schizas2008consensus2}{{104}{}{{}}{{}}}
\bibcite{schizas2008consensus}{{105}{}{{}}{{}}}
\bibcite{schnelle2012compressive}{{106}{}{{}}{{}}}
\bibcite{Sejdinovic2010}{{107}{}{{}}{{}}}
\bibcite{johnsonc8}{{108}{}{{}}{{}}}
\bibcite{shalev2014understanding}{{109}{}{{}}{{}}}
\bibcite{shannon2001mathematical}{{110}{}{{}}{{}}}
\bibcite{shental}{{111}{}{{}}{{}}}
\bibcite{shi2014linear}{{112}{}{{}}{{}}}
\bibcite{Shi2013}{{113}{}{{}}{{}}}
\bibcite{Slavinsky2011}{{114}{}{{}}{{}}}
\bibcite{su2014differential}{{115}{}{{}}{{}}}
\bibcite{sundman2010use}{{116}{}{{}}{{}}}
\bibcite{Sundman2013a}{{117}{}{{}}{{}}}
\bibcite{tan}{{118}{}{{}}{{}}}
\bibcite{tandra2008snr}{{119}{}{{}}{{}}}
\bibcite{taylor1979deconvolution}{{120}{}{{}}{{}}}
\bibcite{tian2006wavelet}{{121}{}{{}}{{}}}
\bibcite{tian2012cyclic}{{122}{}{{}}{{}}}
\bibcite{Tibshirani1996}{{123}{}{{}}{{}}}
\bibcite{tibshirani1996regression}{{124}{}{{}}{{}}}
\bibcite{tropp2007signal}{{125}{}{{}}{{}}}
\bibcite{tropp2010beyond}{{126}{}{{}}{{}}}
\bibcite{unser2000sampling}{{127}{}{{}}{{}}}
\bibcite{verlant2012multiband}{{128}{}{{}}{{}}}
\bibcite{Wadayama}{{129}{}{{}}{{}}}
\bibcite{wen2015efficient}{{130}{}{{}}{{}}}
\bibcite{wen2013improved}{{131}{}{{}}{{}}}
\bibcite{xie2009optimal}{{132}{}{{}}{{}}}
\bibcite{ye2007spectrum}{{133}{}{{}}{{}}}
\bibcite{Yedidia2011}{{134}{}{{}}{{}}}
\bibcite{yoo2012design}{{135}{}{{}}{{}}}
\bibcite{yucek2009survey}{{136}{}{{}}{{}}}
\bibcite{Zhang2011b}{{137}{}{{}}{{}}}
\bibcite{zhang2011adaptive}{{138}{}{{}}{{}}}
\bibcite{Zhang2011a}{{139}{}{{}}{{}}}
\bibcite{zou2005regularization}{{140}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\memsetcounter{lastsheet}{150}
\memsetcounter{lastpage}{142}
