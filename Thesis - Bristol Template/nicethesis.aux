\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\bibstyle{plain}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\par \penalty \@M \unhbox \voidb@x \hbox {}\hfill {\nag@@warning@vi  \bfseries  Page}\par \penalty \@M }
\citation{Strategy2013}
\citation{Burbidge2007}
\citation{Tibshirani1996}
\citation{blumensath2007difference}
\citation{Tibshirani1996}
\citation{Ji2008}
\citation{Ji2008}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{v}{section*.1}}
\citation{atia2}
\@writefile{lof}{\par \penalty \@M \textbf  {{\scshape  Figure} \hfill Page}\par \penalty \@M }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\citation{Strategy2013}
\citation{Strategy2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{figure.1.1}}
\newlabel{spectrumalloc}{{\M@TitleReference {1.1}{A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{A digram of current Spectral allocation \cite {Strategy2013}}{figure.1.1}{}}
\citation{Burbidge2007}
\citation{Burbidge2007}
\citation{Candes2006}
\citation{donoho2}
\citation{Donoho}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{figure.1.2}}
\newlabel{frequtil}{{\M@TitleReference {1.2}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}{figure.1.2}{}}
\citation{mishali2010theory}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Classical Sensing}{11}{chapter.2}}
\newlabel{chap:classical-sensning}{{\M@TitleReference {2}{Classical Sensing}}{11}{Classical Sensing}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{11}{section.2.1}}
\citation{shannon2001mathematical}
\citation{nyquist2002certain}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Classical Sensing}{12}{section.2.2}}
\newlabel{sec:classic-sensing}{{\M@TitleReference {2.2}{Classical Sensing}}{12}{Classical Sensing}{section.2.2}{}}
\citation{nyquist2002certain}
\newlabel{shah}{{\M@TitleReference {2.2}{Classical Sensing}}{13}{Classical Sensing}{equation.2.9}{}}
\citation{yucek2009survey}
\newlabel{h1}{{\M@TitleReference {2.2}{Classical Sensing}}{14}{Classical Sensing}{equation.2.17}{}}
\newlabel{h2}{{\M@TitleReference {2.2}{Classical Sensing}}{14}{Classical Sensing}{equation.2.18}{}}
\citation{yucek2009survey}
\newlabel{likeratio}{{\M@TitleReference {2.2}{Classical Sensing}}{15}{Classical Sensing}{equation.2.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Classical Sensing Techniques}{15}{section.2.3}}
\newlabel{sec:techniques}{{\M@TitleReference {2.3}{Classical Sensing Techniques}}{15}{Classical Sensing Techniques}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Energy Detection}{15}{subsection.2.3.1}}
\newlabel{sec:energy-detection}{{\M@TitleReference {2.3.1}{Energy Detection}}{15}{Energy Detection}{subsection.2.3.1}{}}
\citation{yucek2009survey}
\citation{xie2009optimal}
\citation{hamdi2010impact}
\citation{sahai2004some}
\citation{zhang2011adaptive}
\citation{olivieri2005scalable}
\citation{tandra2008snr}
\citation{oude2011lowering}
\citation{ye2007spectrum}
\citation{kim2007cyclostationary}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Cyclostationary Feature Detection}{16}{subsection.2.3.2}}
\newlabel{cyclic-covarience}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{17}{Cyclostationary Feature Detection}{equation.2.28}{}}
\citation{Ghozzi2006}
\citation{lunden2007spectrum}
\citation{cabric2004implementation}
\citation{vcabric2005physical}
\citation{Ghozzi2006}
\citation{cabric2004implementation}
\citation{yucek2009survey}
\newlabel{c1}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{18}{Cyclostationary Feature Detection}{equation.2.32}{}}
\newlabel{c2}{{\M@TitleReference {2.3.2}{Cyclostationary Feature Detection}}{18}{Cyclostationary Feature Detection}{equation.2.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Matched Filtering}{18}{subsection.2.3.3}}
\citation{bhargavi2010performance}
\citation{oksanen2010characterization}
\citation{cho2015weighted}
\citation{ma2008soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Distributed Approaches to Spectrum Sensing}{19}{subsection.2.3.4}}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{19}|hyperpage}{19}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{19}|hyperpage}{19}
\@@wrindexm@m{nicethesis}{\glossaryentry{FC?\glossentry{fc}|setentrycounter[]{page}\glsnumberformat}{19}|hyperpage}{19}
\@@wrindexm@m{nicethesis}{\glossaryentry{CR?\glossentry{cr}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@@wrindexm@m{nicethesis}{\glossaryentry{PU?\glossentry{pu}|setentrycounter[]{page}\glsnumberformat}{20}|hyperpage}{20}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Limitations}{20}{subsection.2.3.5}}
\newlabel{chap:nyq}{{\M@TitleReference {2.3.5}{Limitations}}{20}{Limitations}{subsection.2.3.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Compressive Sensing}{21}{chapter.3}}
\newlabel{chap:cs}{{\M@TitleReference {3}{Compressive Sensing}}{21}{Compressive Sensing}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{21}{section.3.1}}
\newlabel{sec:csinto}{{\M@TitleReference {3.1}{Introduction}}{21}{Introduction}{section.3.1}{}}
\@@wrindexm@m{nicethesis}{\glossaryentry{CS?\glossentry{cs}|setentrycounter[]{page}\glsnumberformat}{21}|hyperpage}{21}
\citation{unser2000sampling,}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Preliminaries}{22}{section.3.2}}
\newlabel{sec:prelims}{{\M@TitleReference {3.2}{Preliminaries}}{22}{Preliminaries}{section.3.2}{}}
\citation{prony1795essai}
\citation{Caratheodory1907}
\citation{claerbout1973robust}
\citation{taylor1979deconvolution}
\citation{Santosa1986}
\citation{candes2008introduction}
\@@wrindexm@m{nicethesis}{\glossaryentry{CS?\glossentry{cs}|setentrycounter[]{page}\glsnumberformat}{23}|hyperpage}{23}
\newlabel{inner-product-repr}{{\M@TitleReference {3.2}{Preliminaries}}{23}{Preliminaries}{equation.3.1}{}}
\newlabel{vector-repr}{{\M@TitleReference {3.2}{Preliminaries}}{24}{Preliminaries}{equation.3.2}{}}
\newlabel{CSequation}{{\M@TitleReference {3.2}{Preliminaries}}{24}{Preliminaries}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}RIP and Stable Embeddings}{24}{subsection.3.2.1}}
\newlabel{sparse-basis-expanson}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{24}{Sparsity}{equation.3.4}{}}
\newlabel{def:alpha}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{24}{Sparsity}{equation.3.5}{}}
\citation{shalev2014understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{25}{figure.3.1}}
\newlabel{l1l2}{{\M@TitleReference {3.1}{A visualisation of the Compressive Sensing problem as an under-determined system}}{25}{A visualisation of the Compressive Sensing problem as an under-determined system}{figure.3.1}{}}
\newlabel{cs-model}{{\M@TitleReference {3.2.1}{RIP and Stable Embeddings}}{25}{RIP and Stable Embeddings}{equation.3.7}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.2.2}{RIP and Stable Embeddings}}{25}{RIP}{theorem.3.2.2}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.2.2}{RIP and Stable Embeddings}}{25}{RIP}{equation.3.8}{}}
\newlabel{def:d-stable}{{\M@TitleReference {3.2.7}{RIP and Stable Embeddings}}{26}{\(\delta \)-stable embedding}{equation.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Incoherence}{26}{section.3.3}}
\citation{Candes2006}
\citation{davenport2010signal}
\citation{baraniuk2008simple}
\newlabel{minsamples}{{\M@TitleReference {3.3.2}{Incoherence}}{27}{Reconstruction from Compressive measurements \cite {Candes2006}}{equation.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Random Matrix Constructions}{27}{subsection.3.3.1}}
\newlabel{sec:mtx-contruction}{{\M@TitleReference {3.3.1}{Random Matrix Constructions}}{27}{Random Matrix Constructions}{subsection.3.3.1}{}}
\newlabel{cond:norm-pres}{{\M@TitleReference {1}{Random Matrix Constructions}}{27}{Norm preservation}{condition.1}{}}
\newlabel{cond:sub-Gauss}{{\M@TitleReference {2}{Random Matrix Constructions}}{27}{sub-Gaussian}{condition.2}{}}
\citation{baraniuk2008simple}
\citation{baraniuk2008simple}
\newlabel{cond:sub-Gauss concetration}{{3.12}{28}{sub-Gaussian}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Wishart Matrices}{28}{subsection.3.3.2}}
\newlabel{sec:wishart}{{\M@TitleReference {3.3.2}{Wishart Matrices}}{28}{Wishart Matrices}{subsection.3.3.2}{}}
\newlabel{thm:wishart-mean}{{\M@TitleReference {3.3.7}{Wishart Matrices}}{28}{Expected Value}{theorem.3.3.7}{}}
\citation{levequeMatrices}
\citation{Chen1998a}
\citation{tibshirani1996regression}
\newlabel{remark: exp AtA}{{\M@TitleReference {3.3.8}{Wishart Matrices}}{29}{}{theorem.3.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reconstruction Objectives}{29}{section.3.4}}
\newlabel{program:bp}{{3.19}{29}{Reconstruction Objectives}{equation.3.19}{}}
\newlabel{program:lasso}{{3.20}{29}{Reconstruction Objectives}{equation.3.20}{}}
\citation{hoerl1970ridge}
\citation{breiman1995better}
\citation{hastie2005elements}
\citation{Elad2010}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{zou2005regularization}
\newlabel{program:Ridge-regression}{{3.21}{30}{Reconstruction Objectives}{equation.3.21}{}}
\newlabel{program:ell0}{{3.22}{30}{Reconstruction Objectives}{equation.3.22}{}}
\newlabel{soln:lasso}{{3.23}{30}{Reconstruction Objectives}{equation.3.23}{}}
\newlabel{soln:ridge}{{3.24}{30}{Reconstruction Objectives}{equation.3.24}{}}
\newlabel{soln:l0}{{3.25}{30}{Reconstruction Objectives}{equation.3.25}{}}
\newlabel{program:enat}{{3.26}{30}{Reconstruction Objectives}{equation.3.26}{}}
\citation{candes2007dantzig}
\citation{candes2007dantzig}
\citation{bickel2009simultaneous}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{31}{figure.3.2}}
\newlabel{fig:l1l2}{{\M@TitleReference {3.2}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{31}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.3.2}{}}
\newlabel{program:tc}{{3.27}{31}{Reconstruction Objectives}{equation.3.27}{}}
\newlabel{program:danzig}{{3.28}{31}{Reconstruction Objectives}{equation.3.28}{}}
\citation{figueiredo2003algorithm}
\citation{wen2015efficient}
\citation{oxvig2012improving}
\citation{mohimani2010sparse}
\citation{tropp2007signal}
\citation{dai2009subspace}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Reconstruction Algorithms}{32}{section.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Convex Algorithms}{32}{subsection.3.5.1}}
\citation{blumensath2007difference}
\citation{blumensath2007difference}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{33}{figure.3.3}}
\newlabel{alg:IST}{{\M@TitleReference {3.3}{The Iterative Soft Thresholding Algorithm}}{33}{The Iterative Soft Thresholding Algorithm}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The OMP recovery algorithm}}{33}{figure.3.4}}
\newlabel{alg:omp}{{\M@TitleReference {3.4}{The OMP recovery algorithm}}{33}{The OMP recovery algorithm}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Greedy Algorithms}{33}{subsection.3.5.2}}
\citation{goldberger1961stepwise}
\citation{pati1993orthogonal}
\citation{mallat1993matching}
\citation{tropp2007signal}
\citation{wen2013improved}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An illustration of the orthognalisation step of OMP. \cite  {blumensath2007difference}}}{34}{figure.3.5}}
\newlabel{fig:OMP}{{\M@TitleReference {3.5}{An illustration of the orthognalisation step of OMP. \cite  {blumensath2007difference}}}{34}{An illustration of the orthognalisation step of OMP. \cite {blumensath2007difference}}{figure.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Bayesian Algorithms}{34}{subsection.3.5.3}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Baron2010}
\citation{Ji2008}
\citation{Ji2008}
\citation{neal2011mcmc}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The AMP recovery algorithm}}{35}{figure.3.6}}
\newlabel{alg:amp}{{\M@TitleReference {3.6}{The AMP recovery algorithm}}{35}{The AMP recovery algorithm}{figure.3.6}{}}
\citation{Yedidia2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{36}{figure.3.7}}
\newlabel{laplacenormal}{{\M@TitleReference {3.7}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{36}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{36}{figure.3.8}}
\newlabel{fig:bayesiancs}{{\M@TitleReference {3.8}{The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{36}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.8}{}}
\citation{metzler2014denoising}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Compressive Estimation}{37}{section.3.6}}
\newlabel{sec:estimation}{{\M@TitleReference {3.6}{Compressive Estimation}}{37}{Compressive Estimation}{section.3.6}{}}
\newlabel{log-like}{{3.40}{38}{Compressive Estimation}{equation.3.40}{}}
\newlabel{ata}{{3.42}{38}{Compressive Estimation}{equation.3.42}{}}
\newlabel{approx-log-like}{{3.44}{38}{Compressive Estimation}{equation.3.44}{}}
\newlabel{eq: compressive-estimator}{{3.45}{38}{Compressive Estimation}{equation.3.45}{}}
\citation{Zhang2011b}
\citation{Zhang2011b}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Estimating Frequency Spectra}{39}{subsection.3.6.1}}
\newlabel{sec:freq-model}{{\M@TitleReference {3.6.1}{Estimating Frequency Spectra}}{39}{Estimating Frequency Spectra}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Compressive Sensing Architectures}{39}{section.3.7}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {3.7}{Compressive Sensing Architectures}}{39}{Compressive Sensing Architectures}{section.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Modulated Wideband Converter}{39}{subsection.3.7.1}}
\newlabel{sec:MWC}{{\M@TitleReference {3.7.1}{Modulated Wideband Converter}}{39}{Modulated Wideband Converter}{subsection.3.7.1}{}}
\citation{mishali2010theory}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces }}{40}{figure.3.9}}
\newlabel{fig:new_basis_25}{{\M@TitleReference {3.9}{}}{40}{}{figure.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{40}{figure.3.10}}
\newlabel{msevssnr0}{{\M@TitleReference {3.10}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{40}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.3.10}{}}
\citation{kirolos2006analog}
\citation{harms2013constrained}
\newlabel{system}{{3.53}{41}{Modulated Wideband Converter}{equation.3.53}{}}
\citation{yoo2012design}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Random Demodulator}{42}{subsection.3.7.2}}
\citation{Ji2008}
\citation{Ji2008}
\citation{massoud2011efficient}
\citation{harms2013constrained}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{43}{figure.3.11}}
\newlabel{bayesiancs}{{\M@TitleReference {3.11}{The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{43}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.11}{}}
\citation{Slavinsky2011}
\citation{davenport2012pros}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Compressive Multiplexer}{44}{subsection.3.7.3}}
\@@wrindexm@m{nicethesis}{\glossaryentry{CMUX?\glossentry{cmux}|setentrycounter[]{page}\glsnumberformat}{44}|hyperpage}{44}
\@@wrindexm@m{nicethesis}{\glossaryentry{RD?\glossentry{rd}|setentrycounter[]{page}\glsnumberformat}{44}|hyperpage}{44}
\@@wrindexm@m{nicethesis}{\glossaryentry{MWC?\glossentry{mwc}|setentrycounter[]{page}\glsnumberformat}{44}|hyperpage}{44}
\@@wrindexm@m{nicethesis}{\glossaryentry{CMUX?\glossentry{cmux}|setentrycounter[]{page}\glsnumberformat}{44}|hyperpage}{44}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Compressive Sampling for Spectrum Sensing}{44}{section.3.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Noise Folding}{44}{subsection.3.8.1}}
\citation{davenport2012pros}
\citation{davenport2012pros}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Dynamic Range}{45}{subsection.3.8.2}}
\citation{Tian2007}
\citation{Tian2007}
\citation{polo2009compressive}
\citation{Tian2007}
\citation{sundman2010use}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}CS models}{46}{subsection.3.8.3}}
\citation{tian2012cyclic}
\citation{tian2012cyclic}
\citation{romero2016compressive}
\citation{havary2010compressive}
\citation{braun2009signal}
\citation{Davenport2007}
\citation{davenport2010signal}
\citation{Davenport2007}
\citation{lexa2011compressive}
\citation{Duarte}
\citation{Duarte}
\citation{Ma2014b}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Mishali2010}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Zhang2011a}
\citation{Zhang2011b}
\citation{Sundman2013a}
\citation{schizas2008consensus}
\citation{schizas2008consensus2}
\citation{verlant2012multiband}
\citation{bodart2015multiband}
\citation{Davenport2010}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}ADMM}{51}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{51}{section.4.1}}
\newlabel{chap:admm}{{\M@TitleReference {4.1}{Introduction}}{51}{Introduction}{section.4.1}{}}
\citation{mackay2003information}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background}{52}{section.4.2}}
\newlabel{sec:admm}{{\M@TitleReference {4.2}{Background}}{52}{Background}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Preliminary Definitiona}{52}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Lagrangian Formulation}{53}{subsection.4.2.2}}
\citation{Boyd2010a}
\citation{parikh2014proximal}
\citation{rockafellar1976monotone}
\citation{douglas1956numerical}
\citation{eckstein1992douglas}
\citation{Bristow2014}
\citation{heredia2015consensus}
\citation{sawatzky2014proximal}
\citation{o2013splitting}
\citation{Shi2013}
\citation{nishihara2015general}
\citation{ghadimi2015optimal}
\citation{goldstein2014fast}
\citation{chen2016direct}
\newlabel{dual-ascent}{{4.24}{55}{Dual Ascent}{equation.4.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}ADMM}{55}{subsection.4.2.3}}
\citation{nesterov2005smooth}
\newlabel{admm}{{4.25}{56}{ADMM}{equation.4.25}{}}
\newlabel{admm_aug_lagrangian}{{4.26}{56}{ADMM}{equation.4.26}{}}
\newlabel{admm_algo}{{4.28}{56}{ADMM}{equation.4.28}{}}
\newlabel{minl2_aug_lagrangian}{{4.34}{57}{Minimum norm solution of a Linear Equation}{equation.4.34}{}}
\newlabel{minl2_algo}{{4.36}{57}{Minimum norm solution of a Linear Equation}{equation.4.36}{}}
\newlabel{LASSO}{{4.38}{57}{LASSO}{equation.4.38}{}}
\newlabel{LASSO-L0}{{4.39}{57}{LASSO}{equation.4.39}{}}
\newlabel{admm_algo_lasso}{{4.42}{57}{LASSO}{equation.4.42}{}}
\citation{petersen2008matrix}
\newlabel{dellx}{{4.43}{58}{LASSO}{equation.4.43}{}}
\newlabel{optx}{{4.45}{58}{LASSO}{equation.4.45}{}}
\newlabel{dellz-positive}{{4.46}{58}{LASSO}{equation.4.46}{}}
\newlabel{dellz-negative}{{4.47}{58}{LASSO}{equation.4.47}{}}
\newlabel{zbounds}{{4.48}{59}{LASSO}{equation.4.48}{}}
\newlabel{optz}{{4.49}{59}{LASSO}{equation.4.49}{}}
\newlabel{hatx}{{4.52}{59}{}{equation.4.52}{}}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Proximity Operator}{60}{section.4.3}}
\newlabel{sec:prox}{{\M@TitleReference {4.3}{The Proximity Operator}}{60}{The Proximity Operator}{section.4.3}{}}
\newlabel{def:prox_operator}{{\M@TitleReference {4.3.1}{The Proximity Operator}}{60}{Proximity Operator}{theorem.4.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Properties}{60}{section*.2}}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\@writefile{toc}{\contentsline {subsubsection}{Examples}{61}{section*.3}}
\newlabel{eqn:proxl1}{{\M@TitleReference {4.3.9}{Examples}}{61}{\(l_1\) norm}{equation.4.63}{}}
\newlabel{consensus}{{4.3.12}{62}{Consensus}{theorem.4.3.12}{}}
\newlabel{admm_consensus}{{4.3.12}{62}{Consensus}{theorem.4.3.12}{}}
\newlabel{consensus_iterations}{{4.71}{62}{Consensus}{equation.4.71}{}}
\newlabel{simple_consensus_iterations}{{4.73}{62}{Consensus}{equation.4.73}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{63}{section*.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Acceleration}{64}{section.4.4}}
\newlabel{sec:accel}{{\M@TitleReference {4.4}{Acceleration}}{64}{Acceleration}{section.4.4}{}}
\citation{goldstein2014fast}
\citation{su2014differential}
\citation{Zhang2011b}
\citation{mota2013d}
\citation{mota2013d}
\citation{mota2013d}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Optimisation on Graphs}{67}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{67}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Constrained Optimisation on Graphs}{68}{section.5.2}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {5.2}{Constrained Optimisation on Graphs}}{68}{Constrained Optimisation on Graphs}{section.5.2}{}}
\newlabel{eq:system}{{\M@TitleReference {5.2}{Constrained Optimisation on Graphs}}{68}{Constrained Optimisation on Graphs}{equation.5.1}{}}
\newlabel{constrainedbp}{{5.3}{68}{Constrained Optimisation on Graphs}{equation.5.3}{}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{barxc}{{5.4}{69}{}{equation.5.4}{}}
\newlabel{compact-constraints}{{5.5}{69}{Constrained Optimisation on Graphs}{equation.5.5}{}}
\newlabel{constrainedbp1}{{5.6}{69}{Constrained Optimisation on Graphs}{equation.5.6}{}}
\newlabel{aug-lagrange}{{5.7}{69}{Constrained Optimisation on Graphs}{equation.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of a network}}{70}{figure.5.1}}
\newlabel{efig:ex-network}{{\M@TitleReference {5.1}{An example of a network}}{70}{An example of a network}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{70}{figure.5.2}}
\newlabel{fig:incidence-matrix}{{\M@TitleReference {5.2}{The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{70}{The incidence matrix associated with Figure \eqref {efig:ex-network}}{figure.5.2}{}}
\newlabel{generic-iterations}{{5.11}{71}{Constrained Optimisation on Graphs}{equation.5.11}{}}
\newlabel{dadmm_algo}{{5.16}{72}{}{equation.5.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}DADMM-Lasso}{72}{subsection.5.2.1}}
\newlabel{sec:lasso-algo}{{\M@TitleReference {5.2.1}{DADMM-Lasso}}{72}{DADMM-Lasso}{subsection.5.2.1}{}}
\newlabel{LASSO-chapter5}{{5.17}{72}{DADMM-Lasso}{equation.5.17}{}}
\citation{cotter2005sparse}
\newlabel{dadmm-lasso-lag}{{5.21}{73}{DADMM-Lasso}{equation.5.21}{}}
\newlabel{dadmm_algo_lasso}{{5.25}{73}{}{equation.5.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}DADMM-MMV}{73}{subsection.5.2.2}}
\newlabel{sec:mmv-algo}{{\M@TitleReference {5.2.2}{DADMM-MMV}}{73}{DADMM-MMV}{subsection.5.2.2}{}}
\@@wrindexm@m{nicethesis}{\glossaryentry{MMV?\glossentry{mmv}|setentrycounter[]{page}\glsnumberformat}{73}|hyperpage}{73}
\@@wrindexm@m{nicethesis}{\glossaryentry{MMV?\glossentry{mmv}|setentrycounter[]{page}\glsnumberformat}{73}|hyperpage}{73}
\newlabel{program:bpmmv}{{5.27}{73}{DADMM-MMV}{equation.5.27}{}}
\newlabel{program:lassommv}{{5.28}{73}{DADMM-MMV}{equation.5.28}{}}
\citation{Boyd2010}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{74}{section.5.3}}
\newlabel{sec:results}{{\M@TitleReference {5.3}{Results}}{74}{Results}{section.5.3}{}}
\@@wrindexm@m{nicethesis}{\glossaryentry{SNR?\glossentry{snr}|setentrycounter[]{page}\glsnumberformat}{74}|hyperpage}{74}
\citation{ling2015dlm}
\newlabel{dlm_algo}{{5.37}{75}{}{equation.5.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Conclusions}{75}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{76}{figure.5.3}}
\newlabel{ch5:fig:differentLambda}{{\M@TitleReference {5.3}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{76}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{figure.5.4}}
\newlabel{ch5:fig:erroriterations}{{\M@TitleReference {5.4}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.4}{}}
\newlabel{chap:dist-opt}{{\M@TitleReference {5.4}{Conclusions}}{76}{Conclusions}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations.}}{77}{figure.5.5}}
\newlabel{ch5:fig:l1l0}{{\M@TitleReference {5.5}{The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations.}}{77}{The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations. \(\lambda _{\ell _1} = P\mathchoice {\setbox 0=\hbox {$\displaystyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\textstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptscriptstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}\), \(\lambda _{\ell _0} = 1000\lambda _{\ell _1}\) }}{77}{figure.5.6}}
\newlabel{ch5:fig:l1l0-long}{{\M@TitleReference {5.6}{The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations. \(\lambda _{\ell _1} = P\mathchoice {\setbox 0=\hbox {$\displaystyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\textstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptscriptstyle \sqrt  {2\qopname  \relax o{log}{n}\tmspace  +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}\), \(\lambda _{\ell _0} = 1000\lambda _{\ell _1}\) }}{77}{The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations. \(\lambda _{\ell _1} = P\sqrt {2\log {n}}\), \(\lambda _{\ell _0} = 1000\lambda _{\ell _1}\) }{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces }}{78}{figure.5.7}}
\newlabel{ch5:dlm1}{{\M@TitleReference {5.7}{}}{78}{}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces }}{78}{figure.5.8}}
\newlabel{ch5:dlm2}{{\M@TitleReference {5.8}{}}{78}{}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces }}{79}{figure.5.9}}
\newlabel{ch5:fig:mmv-orig}{{\M@TitleReference {5.9}{}}{79}{}{figure.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces }}{79}{figure.5.10}}
\newlabel{ch5:fig:mmv-recon}{{\M@TitleReference {5.10}{}}{79}{}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces }}{80}{figure.5.11}}
\newlabel{ch5:aty}{{\M@TitleReference {5.11}{}}{80}{}{figure.5.11}{}}
\citation{tian2006wavelet}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Sensing with Heaviside Basis}{81}{chapter.6}}
\newlabel{chap:rect-basis}{{\M@TitleReference {6}{Sensing with Heaviside Basis}}{81}{Sensing with Heaviside Basis}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{81}{section.6.1}}
\citation{tian2006wavelet}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Signal Model}{83}{section.6.2}}
\newlabel{sec:sig-model}{{\M@TitleReference {6.2}{Signal Model}}{83}{Signal Model}{section.6.2}{}}
\newlabel{basis}{{6.1}{83}{Heaviside Basis}{equation.6.1}{}}
\newlabel{lower-L}{{\M@TitleReference {6.2.1}{Signal Model}}{83}{Heaviside Basis}{equation.6.2}{}}
\newlabel{inv-L}{{\M@TitleReference {6.2.2}{Signal Model}}{83}{}{equation.6.3}{}}
\newlabel{basis-expansion}{{6.4}{83}{}{equation.6.4}{}}
\newlabel{def:a}{{\M@TitleReference {6.2.4}{Signal Model}}{84}{}{equation.6.5}{}}
\newlabel{def:Fmtx}{{6.11}{84}{}{equation.6.11}{}}
\newlabel{eq:h-est}{{6.20}{86}{Cumulative Sum}{equation.6.20}{}}
\newlabel{ex:single-rect}{{\M@TitleReference {6.2.9}{Signal Model}}{86}{Single Rectangle}{theorem.6.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces A single rectangle signal, simmilar to example \ref  {ex:single-rect}. This signal is used as an illustrative demonstration for the method developed in this chapter.}}{87}{figure.6.1}}
\newlabel{fig:rectangle}{{\M@TitleReference {6.1}{A single rectangle signal, simmilar to example \ref  {ex:single-rect}. This signal is used as an illustrative demonstration for the method developed in this chapter.}}{87}{A single rectangle signal, simmilar to example \ref {ex:single-rect}. This signal is used as an illustrative demonstration for the method developed in this chapter}{figure.6.1}{}}
\newlabel{key-step}{{6.31}{87}{Signal Model}{equation.6.31}{}}
\newlabel{ss-estimator}{{6.33}{88}{Signal Model}{equation.6.33}{}}
\newlabel{fig:hhat}{{\M@TitleReference {6.2}{Signal Model}}{88}{Signal Model}{equation.6.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces An example of the estimate \(\mathaccentV {hat}002{h}\) for a single rectangle signal (in red) compared to the true cumulative vector \(h\) (in blue). Note how the estimate tracks the true signal, with little deviation.}}{88}{figure.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results for Compressive Inference}{89}{section.6.3}}
\newlabel{ci-results}{{\M@TitleReference {6.3}{Results for Compressive Inference}}{89}{Results for Compressive Inference}{section.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Synthetic signal used for experiments in this section}}{89}{figure.6.3}}
\newlabel{ci-sig}{{\M@TitleReference {6.3}{Synthetic signal used for experiments in this section}}{89}{Synthetic signal used for experiments in this section}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in table \ref  {alg:single-slot}), for a reconstruction of a the signal in \ref  {ci-sig}. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{figure.6.4}}
\newlabel{different_k}{{\M@TitleReference {6.4}{ROC curves for the single-shot algorithm (as outlined in table \ref  {alg:single-slot}), for a reconstruction of a the signal in \ref  {ci-sig}. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{ROC curves for the single-shot algorithm (as outlined in table \ref {alg:single-slot}), for a reconstruction of a the signal in \ref {ci-sig}. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve}{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-4.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{figure.6.5}}
\newlabel{different_k_4.5}{{\M@TitleReference {6.5}{ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-4.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \(\re ^{1000}\), with noise added at an SNR of \(-4.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{figure.6.6}}
\newlabel{different_k_10.5}{{\M@TitleReference {6.6}{ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \(\re ^{1000}\), with noise added at an SNR of \(-10.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve}{figure.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-18\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{figure.6.7}}
\newlabel{different_k_18}{{\M@TitleReference {6.7}{ROC curves for the single-shot algorithm (as outlined in \ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-18\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \(\re ^{1000}\), with noise added at an SNR of \(-18\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve}{figure.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces SNR vs AUC for different levels of undersampling (as indicated in the legend)}}{92}{figure.6.8}}
\newlabel{snrauc}{{\M@TitleReference {6.8}{SNR vs AUC for different levels of undersampling (as indicated in the legend)}}{92}{SNR vs AUC for different levels of undersampling (as indicated in the legend)}{figure.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces SNR vs AUC for different levels of undersampling (as indicated in the legend), this time zoomed out from figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {snrauc}\unskip \@@italiccorr )}}}}{92}{figure.6.9}}
\newlabel{snrauc_pan}{{\M@TitleReference {6.9}{SNR vs AUC for different levels of undersampling (as indicated in the legend), this time zoomed out from figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {snrauc}\unskip \@@italiccorr )}}}}{92}{SNR vs AUC for different levels of undersampling (as indicated in the legend), this time zoomed out from figure \eqref {snrauc}}{figure.6.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Benchmarking the Algorithm}{93}{subsection.6.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in table ,l;/l\ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{93}{figure.6.10}}
\newlabel{oracle-compare}{{\M@TitleReference {6.10}{ROC curves for the single-shot algorithm (as outlined in table ,l;/l\ref  {alg:single-slot}), for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{93}{ROC curves for the single-shot algorithm (as outlined in table ,l;/l\ref {alg:single-slot}), for a signal in \(\re ^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve}{figure.6.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces ROC curves for the single-shot algorithm with correct changepoints, for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{94}{figure.6.11}}
\newlabel{oracle}{{\M@TitleReference {6.11}{ROC curves for the single-shot algorithm with correct changepoints, for a signal in \({\math@bb  {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{94}{ROC curves for the single-shot algorithm with correct changepoints, for a signal in \(\re ^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve}{figure.6.11}{}}
\citation{Chen1998}
\citation{bazerque2008}
\citation{bazerque2008}
\citation{shi2014linear}
\citation{nishihara2015general}
\citation{su2014differential}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Results for Distributed Sensing}{95}{section.6.4}}
\newlabel{sec:dist-results}{{\M@TitleReference {6.4}{Results for Distributed Sensing}}{95}{Results for Distributed Sensing}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{96}{figure.6.12}}
\newlabel{different_sigs}{{\M@TitleReference {6.12}{Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{96}{Left to right: (a) The original signal. (b) The gradient \eqref {def:a} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }{figure.6.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{97}{figure.6.13}}
\newlabel{msevssnr0}{{\M@TitleReference {6.13}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{97}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands}{figure.6.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{97}{figure.6.14}}
\newlabel{fig:differentLambda}{{\M@TitleReference {6.14}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{97}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations}{figure.6.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results on OFCOM data}{98}{section.6.5}}
\newlabel{ofcom-results}{{\M@TitleReference {6.5}{Results on OFCOM data}}{98}{Results on OFCOM data}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Data Set}{98}{subsection.6.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Results: Distributed Estimation with Heaviside Basis}{98}{subsection.6.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces }}{99}{figure.6.15}}
\newlabel{fig:hvb}{{\M@TitleReference {6.15}{}}{99}{}{figure.6.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces }}{99}{figure.6.16}}
\newlabel{fig:hvb}{{\M@TitleReference {6.16}{}}{99}{}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces }}{100}{figure.6.17}}
\newlabel{fig:hvb}{{\M@TitleReference {6.17}{}}{100}{}{figure.6.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces }}{100}{figure.6.18}}
\newlabel{fig:hvb}{{\M@TitleReference {6.18}{}}{100}{}{figure.6.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Compressive Estimation}{101}{subsection.6.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{101}{figure.6.19}}
\newlabel{fig:hvb}{{\M@TitleReference {6.19}{Example of classification with OFCOM data, 35 changepoints}}{101}{Example of classification with OFCOM data, 35 changepoints}{figure.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Example of classification with OFCOM data, 85 changepoints}}{101}{figure.6.20}}
\newlabel{fig:hvb}{{\M@TitleReference {6.20}{Example of classification with OFCOM data, 85 changepoints}}{101}{Example of classification with OFCOM data, 85 changepoints}{figure.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{102}{figure.6.21}}
\newlabel{fig:hvb}{{\M@TitleReference {6.21}{Example of classification with OFCOM data, 55 changepoints}}{102}{Example of classification with OFCOM data, 55 changepoints}{figure.6.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{102}{figure.6.22}}
\newlabel{fig:hvb}{{\M@TitleReference {6.22}{Example of classification with OFCOM data, 55 changepoints}}{102}{Example of classification with OFCOM data, 55 changepoints}{figure.6.22}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Sensing with Heaviside Basis in time and frequency}{103}{chapter.7}}
\newlabel{chap:2d-rect-basis}{{\M@TitleReference {7}{Sensing with Heaviside Basis in time and frequency}}{103}{Sensing with Heaviside Basis in time and frequency}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{103}{section.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Time-Frequency Model}{103}{section.7.2}}
\newlabel{time-freq basis}{{7.1}{104}{}{equation.7.1}{}}
\newlabel{time-basis}{{7.2}{104}{}{equation.7.2}{}}
\newlabel{def:time-freq matrix}{{7.7}{105}{}{equation.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Compressive Method}{107}{subsection.7.2.1}}
\newlabel{period vector}{{\M@TitleReference {7.2.10}{Compressive Method}}{108}{Period-change vector}{equation.7.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Example multiple time-slot signal}}{108}{figure.7.1}}
\newlabel{fig:gt}{{\M@TitleReference {7.1}{Example multiple time-slot signal}}{108}{Example multiple time-slot signal}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Compressive Measurements of the previous signal \ref  {gt}}}{108}{figure.7.2}}
\newlabel{fig:ygt}{{\M@TitleReference {7.2}{Compressive Measurements of the previous signal \ref  {gt}}}{108}{Compressive Measurements of the previous signal \ref {gt}}{figure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Period-change vector for the signal \ref  {gt}}}{109}{figure.7.3}}
\newlabel{fig:period-vec}{{\M@TitleReference {7.3}{Period-change vector for the signal \ref  {gt}}}{109}{Period-change vector for the signal \ref {gt}}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Procedure for estimating occupancy of frequency spectra with multiple time slots}}{109}{figure.7.4}}
\newlabel{alg:multiple-shot}{{\M@TitleReference {7.4}{Procedure for estimating occupancy of frequency spectra with multiple time slots}}{109}{Procedure for estimating occupancy of frequency spectra with multiple time slots}{figure.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Non-compressive Method}{110}{subsection.7.2.2}}
\newlabel{eq:w}{{\M@TitleReference {7.2.11}{Non-compressive Method}}{110}{Time-difference Vector}{equation.7.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{111}{section.7.3}}
\newlabel{sec:results}{{\M@TitleReference {7.3}{Results}}{111}{Results}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Synthetic Data}{111}{subsection.7.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces ROC curves for the multi-shot algorithm (as outlined in \ref  {alg:multiple-slot}), for a signal in \({\math@bb  {R}}^{300}\), over 5 time slots. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve. }}{111}{figure.7.5}}
\newlabel{multi_slot_different_k}{{\M@TitleReference {7.5}{ROC curves for the multi-shot algorithm (as outlined in \ref  {alg:multiple-slot}), for a signal in \({\math@bb  {R}}^{300}\), over 5 time slots. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve. }}{111}{ROC curves for the multi-shot algorithm (as outlined in \ref {alg:multiple-slot}), for a signal in \(\re ^{300}\), over 5 time slots. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve. }{figure.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}OFCOM Data}{111}{subsection.7.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces }}{112}{figure.7.6}}
\newlabel{good_sythetic_multi}{{\M@TitleReference {7.6}{}}{112}{}{figure.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces }}{112}{figure.7.7}}
\newlabel{bad_synthetic_multi}{{\M@TitleReference {7.7}{}}{112}{}{figure.7.7}{}}
\citation{li5}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Group Testing}{113}{chapter.8}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction and notation}{113}{section.8.1}}
\citation{Dorfman1943}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Group Testing}{114}{subsection.8.1.1}}
\citation{atia2}
\citation{atia2}
\citation{du}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{115}{figure.8.1}}
\newlabel{bayesiancs}{{\M@TitleReference {8.1}{The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{115}{The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}{figure.8.1}{}}
\citation{Hwang1972}
\citation{Aldridge2013}
\citation{Chan2011}
\citation{Baldassini2013}
\newlabel{hwangbound}{{\M@TitleReference {8.1.1}{Bounds}}{117}{Bounds}{equation.8.9}{}}
\newlabel{compbound}{{\M@TitleReference {8.1.1}{Bounds}}{117}{Bounds}{equation.8.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Compressive Sensing}{117}{section*.8}}
\citation{Emma}
\citation{Sejdinovic2010}
\citation{Wadayama}
\citation{Baldassini2013}
\citation{dorfman}
\citation{dorfman}
\citation{du,malyutov}
\citation{malyutov}
\citation{atia,johnsonc8,johnson33}
\citation{Wadayama}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}The Probabilistic group testing problem}{118}{subsection.8.1.2}}
\citation{li5}
\citation{atia2}
\citation{dorfman}
\citation{shental}
\citation{atia,johnsonc10,johnson33,tan}
\citation{atia,tan}
\citation{johnsonc10}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Group testing capacity}{119}{subsection.8.1.3}}
\newlabel{def:capacity}{{\M@TitleReference {8.1.1}{Group testing capacity}}{119}{}{theorem.8.1.1}{}}
\newlabel{eq:lower}{{8.11}{119}{}{equation.8.11}{}}
\citation{johnsonc10}
\citation{li5}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{hwang}
\citation{du}
\citation{johnsonc10}
\citation{}
\citation{li5}
\citation{li5}
\citation{hwang}
\newlabel{eq:upper}{{8.12}{120}{}{equation.8.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Main results}{120}{subsection.8.1.4}}
\newlabel{thm:mainold}{{\M@TitleReference {8.1.4}{Main results}}{120}{\cite {johnsonc10}}{theorem.8.1.4}{}}
\newlabel{eq:bja}{{8.13}{120}{Main results}{equation.8.13}{}}
\newlabel{cor:main}{{\M@TitleReference {8.1.5}{Main results}}{120}{}{theorem.8.1.5}{}}
\citation{li5}
\citation{johnsonc10}
\citation{li5}
\citation{li5}
\citation{li5}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Algorithms and existing results}{121}{section.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Upper bounds on success probability}{121}{subsection.8.2.1}}
\newlabel{sec:ub}{{\M@TitleReference {8.2.1}{Upper bounds on success probability}}{121}{Upper bounds on success probability}{subsection.8.2.1}{}}
\newlabel{thm:upper}{{\M@TitleReference {8.2.1}{Upper bounds on success probability}}{121}{}{theorem.8.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Binary search algorithms}{121}{subsection.8.2.2}}
\citation{li5}
\citation{li5}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{122}{figure.8.2}}
\newlabel{thm:lower}{{\M@TitleReference {8.2.3}{Binary search algorithms}}{122}{}{theorem.8.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Summary of our contribution}{122}{subsection.8.2.3}}
\newlabel{sec:algo}{{\M@TitleReference {8.2.3}{Summary of our contribution}}{122}{Summary of our contribution}{subsection.8.2.3}{}}
\citation{aksoylar,tan}
\citation{Candes2006,donoho2}
\citation{Candes2006,donoho2}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Wider context: sparse inference problems}{123}{subsection.8.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Analysis and new bounds}{123}{section.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Searching a set of bounded ratio}{123}{subsection.8.3.1}}
\newlabel{sec:boundedratio}{{\M@TitleReference {8.3.1}{Searching a set of bounded ratio}}{123}{Searching a set of bounded ratio}{subsection.8.3.1}{}}
\newlabel{cond:ratio}{{\M@TitleReference {3}{Searching a set of bounded ratio}}{123}{Bounded Ratio Condition}{condition.3}{}}
\newlabel{eq:ratio}{{8.14}{123}{Bounded Ratio Condition}{equation.8.14}{}}
\newlabel{lem:sfstep}{{\M@TitleReference {8.3.1}{Searching a set of bounded ratio}}{123}{}{theorem.8.3.1}{}}
\newlabel{eq:depth}{{8.15}{123}{}{equation.8.15}{}}
\newlabel{eq:setbd}{{8.16}{124}{Searching a set of bounded ratio}{equation.8.16}{}}
\newlabel{eq:lengthbd}{{\M@TitleReference {8.17}{Searching a set of bounded ratio}}{124}{Searching a set of bounded ratio}{equation.8.17}{}}
\newlabel{rem:algo}{{\M@TitleReference {8.3.2}{Searching a set of bounded ratio}}{124}{}{theorem.8.3.2}{}}
\newlabel{lem:expset}{{\M@TitleReference {8.3.3}{Searching a set of bounded ratio}}{124}{}{theorem.8.3.3}{}}
\newlabel{eq:tbds}{{8.18}{124}{}{equation.8.18}{}}
\citation{li5}
\citation{li5}
\newlabel{eq:testsperdef}{{\M@TitleReference {8.19}{Searching a set of bounded ratio}}{125}{Searching a set of bounded ratio}{equation.8.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Discarding low probability items}{125}{subsection.8.3.2}}
\newlabel{sec:discard}{{\M@TitleReference {8.3.2}{Discarding low probability items}}{125}{Discarding low probability items}{subsection.8.3.2}{}}
\newlabel{lem:thresh}{{\M@TitleReference {8.3.4}{Discarding low probability items}}{125}{}{theorem.8.3.4}{}}
\newlabel{eq:thetadef}{{8.20}{125}{}{equation.8.20}{}}
\newlabel{eq:setpstar}{{8.21}{125}{}{equation.8.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Searching the entire set}{125}{subsection.8.3.3}}
\newlabel{eq:bincount}{{8.22}{126}{Searching the entire set}{equation.8.22}{}}
\newlabel{def:full}{{\M@TitleReference {8.3.5}{Searching the entire set}}{126}{}{theorem.8.3.5}{}}
\newlabel{prop:splitting}{{\M@TitleReference {8.3.6}{Searching the entire set}}{126}{}{theorem.8.3.6}{}}
\newlabel{it:count}{{\M@TitleReference {1}{Searching the entire set}}{126}{Searching the entire set}{Item.18}{}}
\newlabel{eq:counting}{{8.23}{126}{Searching the entire set}{equation.8.23}{}}
\citation{petrov}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Bounding the expected number of tests}{127}{subsection.8.3.4}}
\newlabel{sec:expectation}{{\M@TitleReference {8.3.4}{Bounding the expected number of tests}}{127}{Bounding the expected number of tests}{subsection.8.3.4}{}}
\newlabel{prop:overall}{{\M@TitleReference {8.3.7}{Bounding the expected number of tests}}{127}{}{theorem.8.3.7}{}}
\newlabel{eq:tbd}{{8.24}{127}{}{equation.8.24}{}}
\newlabel{eq:total}{{\M@TitleReference {8.25}{Bounding the expected number of tests}}{127}{Bounding the expected number of tests}{equation.8.25}{}}
\newlabel{eq:toopt}{{\M@TitleReference {8.25}{Bounding the expected number of tests}}{127}{Bounding the expected number of tests}{equation.8.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Controlling the error probabilities}{127}{subsection.8.3.5}}
\newlabel{sec:main}{{\M@TitleReference {8.3.5}{Controlling the error probabilities}}{127}{Controlling the error probabilities}{subsection.8.3.5}{}}
\newlabel{thm:bernstein}{{\M@TitleReference {8.3.8}{Controlling the error probabilities}}{127}{Bernstein}{theorem.8.3.8}{}}
\newlabel{eq:bernstein}{{8.26}{127}{Bernstein}{equation.8.26}{}}
\newlabel{thm:main}{{\M@TitleReference {8.3.9}{Controlling the error probabilities}}{128}{}{theorem.8.3.9}{}}
\newlabel{eq:tnec}{{8.27}{128}{}{equation.8.27}{}}
\newlabel{it:part1}{{\M@TitleReference {1}{Controlling the error probabilities}}{128}{}{Item.21}{}}
\newlabel{eq:errorprob}{{8.28}{128}{}{equation.8.28}{}}
\newlabel{it:part2}{{\M@TitleReference {2}{Controlling the error probabilities}}{128}{}{Item.22}{}}
\newlabel{eq:vbd}{{8.29}{128}{Controlling the error probabilities}{equation.8.29}{}}
\citation{li5}
\newlabel{eq:ratio2}{{\M@TitleReference {8.30}{Controlling the error probabilities}}{129}{Controlling the error probabilities}{equation.8.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Results}{129}{section.8.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{130}{figure.8.3}}
\newlabel{ubvslb}{{\M@TitleReference {8.3}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{130}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}{figure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{130}{figure.8.4}}
\newlabel{testsvsalpha}{{\M@TitleReference {8.4}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{130}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }{figure.8.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Discussion}{130}{section.8.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{131}{figure.8.5}}
\newlabel{testsvstheta}{{\M@TitleReference {8.5}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{131}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}{figure.8.5}{}}
\bibdata{thesis2}
\bibcite{aksoylar}{{1}{}{{}}{{}}}
\bibcite{johnson33}{{2}{}{{}}{{}}}
\bibcite{atia2}{{3}{}{{}}{{}}}
\bibcite{atia}{{4}{}{{}}{{}}}
\bibcite{johnsonc10}{{5}{}{{}}{{}}}
\bibcite{baraniuk2008simple}{{6}{}{{}}{{}}}
\bibcite{Baron2010}{{7}{}{{}}{{}}}
\bibcite{bazerque2008}{{8}{}{{}}{{}}}
\bibcite{bhargavi2010performance}{{9}{}{{}}{{}}}
\bibcite{bickel2009simultaneous}{{10}{}{{}}{{}}}
\bibcite{blumensath2007difference}{{11}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{135}{section*.10}}
\bibcite{bodart2015multiband}{{12}{}{{}}{{}}}
\bibcite{Boyd2010a}{{13}{}{{}}{{}}}
\bibcite{braun2009signal}{{14}{}{{}}{{}}}
\bibcite{breiman1995better}{{15}{}{{}}{{}}}
\bibcite{Bristow2014}{{16}{}{{}}{{}}}
\bibcite{Burbidge2007}{{17}{}{{}}{{}}}
\bibcite{vcabric2005physical}{{18}{}{{}}{{}}}
\bibcite{cabric2004implementation}{{19}{}{{}}{{}}}
\bibcite{Emma}{{20}{}{{}}{{}}}
\bibcite{candes2007dantzig}{{21}{}{{}}{{}}}
\bibcite{Candes2006}{{22}{}{{}}{{}}}
\bibcite{candes2008introduction}{{23}{}{{}}{{}}}
\bibcite{Caratheodory1907}{{24}{}{{}}{{}}}
\bibcite{Chan2011}{{25}{}{{}}{{}}}
\bibcite{chen2016direct}{{26}{}{{}}{{}}}
\bibcite{Chen1998a}{{27}{}{{}}{{}}}
\bibcite{Chen1998}{{28}{}{{}}{{}}}
\bibcite{cho2015weighted}{{29}{}{{}}{{}}}
\bibcite{claerbout1973robust}{{30}{}{{}}{{}}}
\bibcite{cotter2005sparse}{{31}{}{{}}{{}}}
\bibcite{dai2009subspace}{{32}{}{{}}{{}}}
\bibcite{Davenport2010}{{33}{}{{}}{{}}}
\bibcite{davenport2010signal}{{34}{}{{}}{{}}}
\bibcite{Davenport2007}{{35}{}{{}}{{}}}
\bibcite{davenport2012pros}{{36}{}{{}}{{}}}
\bibcite{prony1795essai}{{37}{}{{}}{{}}}
\bibcite{donoho2}{{38}{}{{}}{{}}}
\bibcite{Donoho}{{39}{}{{}}{{}}}
\bibcite{Dorfman1943}{{40}{}{{}}{{}}}
\bibcite{douglas1956numerical}{{41}{}{{}}{{}}}
\bibcite{du}{{42}{}{{}}{{}}}
\bibcite{Duarte}{{43}{}{{}}{{}}}
\bibcite{eckstein1992douglas}{{44}{}{{}}{{}}}
\bibcite{Elad2010}{{45}{}{{}}{{}}}
\bibcite{figueiredo2003algorithm}{{46}{}{{}}{{}}}
\bibcite{ghadimi2015optimal}{{47}{}{{}}{{}}}
\bibcite{Ghozzi2006}{{48}{}{{}}{{}}}
\bibcite{Tian2007}{{49}{}{{}}{{}}}
\bibcite{goldberger1961stepwise}{{50}{}{{}}{{}}}
\bibcite{goldstein2014fast}{{51}{}{{}}{{}}}
\bibcite{hamdi2010impact}{{52}{}{{}}{{}}}
\bibcite{harms2013constrained}{{53}{}{{}}{{}}}
\bibcite{hastie2005elements}{{54}{}{{}}{{}}}
\bibcite{havary2010compressive}{{55}{}{{}}{{}}}
\bibcite{heredia2015consensus}{{56}{}{{}}{{}}}
\bibcite{hoerl1970ridge}{{57}{}{{}}{{}}}
\bibcite{hwang}{{58}{}{{}}{{}}}
\bibcite{Ji2008}{{59}{}{{}}{{}}}
\bibcite{kim2007cyclostationary}{{60}{}{{}}{{}}}
\bibcite{kirolos2006analog}{{61}{}{{}}{{}}}
\bibcite{levequeMatrices}{{62}{}{{}}{{}}}
\bibcite{lexa2011compressive}{{63}{}{{}}{{}}}
\bibcite{li5}{{64}{}{{}}{{}}}
\bibcite{ling2015dlm}{{65}{}{{}}{{}}}
\bibcite{lunden2007spectrum}{{66}{}{{}}{{}}}
\bibcite{}{{67}{}{{}}{{}}}
\bibcite{Ma2014b}{{68}{}{{}}{{}}}
\bibcite{mackay2003information}{{69}{}{{}}{{}}}
\bibcite{mallat1993matching}{{70}{}{{}}{{}}}
\bibcite{malyutov}{{71}{}{{}}{{}}}
\bibcite{massoud2011efficient}{{72}{}{{}}{{}}}
\bibcite{metzler2014denoising}{{73}{}{{}}{{}}}
\bibcite{Mishali2010}{{74}{}{{}}{{}}}
\bibcite{mishali2010theory}{{75}{}{{}}{{}}}
\bibcite{mohimani2010sparse}{{76}{}{{}}{{}}}
\bibcite{moreau1965proximite}{{77}{}{{}}{{}}}
\bibcite{mota2013d}{{78}{}{{}}{{}}}
\bibcite{neal2011mcmc}{{79}{}{{}}{{}}}
\bibcite{nesterov2005smooth}{{80}{}{{}}{{}}}
\bibcite{nishihara2015general}{{81}{}{{}}{{}}}
\bibcite{nyquist2002certain}{{82}{}{{}}{{}}}
\bibcite{o2013splitting}{{83}{}{{}}{{}}}
\bibcite{oksanen2010characterization}{{84}{}{{}}{{}}}
\bibcite{olivieri2005scalable}{{85}{}{{}}{{}}}
\bibcite{oude2011lowering}{{86}{}{{}}{{}}}
\bibcite{oxvig2012improving}{{87}{}{{}}{{}}}
\bibcite{parikh2014proximal}{{88}{}{{}}{{}}}
\bibcite{pati1993orthogonal}{{89}{}{{}}{{}}}
\bibcite{petersen2008matrix}{{90}{}{{}}{{}}}
\bibcite{petrov}{{91}{}{{}}{{}}}
\bibcite{polo2009compressive}{{92}{}{{}}{{}}}
\bibcite{rockafellar1976monotone}{{93}{}{{}}{{}}}
\bibcite{romero2016compressive}{{94}{}{{}}{{}}}
\bibcite{sahai2004some}{{95}{}{{}}{{}}}
\bibcite{Santosa1986}{{96}{}{{}}{{}}}
\bibcite{sawatzky2014proximal}{{97}{}{{}}{{}}}
\bibcite{schizas2008consensus2}{{98}{}{{}}{{}}}
\bibcite{schizas2008consensus}{{99}{}{{}}{{}}}
\bibcite{Sejdinovic2010}{{100}{}{{}}{{}}}
\bibcite{johnsonc8}{{101}{}{{}}{{}}}
\bibcite{shalev2014understanding}{{102}{}{{}}{{}}}
\bibcite{shannon2001mathematical}{{103}{}{{}}{{}}}
\bibcite{shental}{{104}{}{{}}{{}}}
\bibcite{shi2014linear}{{105}{}{{}}{{}}}
\bibcite{Shi2013}{{106}{}{{}}{{}}}
\bibcite{Slavinsky2011}{{107}{}{{}}{{}}}
\bibcite{su2014differential}{{108}{}{{}}{{}}}
\bibcite{sundman2010use}{{109}{}{{}}{{}}}
\bibcite{Sundman2013a}{{110}{}{{}}{{}}}
\bibcite{tan}{{111}{}{{}}{{}}}
\bibcite{tandra2008snr}{{112}{}{{}}{{}}}
\bibcite{taylor1979deconvolution}{{113}{}{{}}{{}}}
\bibcite{tian2006wavelet}{{114}{}{{}}{{}}}
\bibcite{tian2012cyclic}{{115}{}{{}}{{}}}
\bibcite{Tibshirani1996}{{116}{}{{}}{{}}}
\bibcite{tibshirani1996regression}{{117}{}{{}}{{}}}
\bibcite{tropp2007signal}{{118}{}{{}}{{}}}
\bibcite{unser2000sampling}{{119}{}{{}}{{}}}
\bibcite{verlant2012multiband}{{120}{}{{}}{{}}}
\bibcite{Wadayama}{{121}{}{{}}{{}}}
\bibcite{wen2015efficient}{{122}{}{{}}{{}}}
\bibcite{wen2013improved}{{123}{}{{}}{{}}}
\bibcite{xie2009optimal}{{124}{}{{}}{{}}}
\bibcite{ye2007spectrum}{{125}{}{{}}{{}}}
\bibcite{Yedidia2011}{{126}{}{{}}{{}}}
\bibcite{yoo2012design}{{127}{}{{}}{{}}}
\bibcite{yucek2009survey}{{128}{}{{}}{{}}}
\bibcite{Zhang2011b}{{129}{}{{}}{{}}}
\bibcite{zhang2011adaptive}{{130}{}{{}}{{}}}
\bibcite{Zhang2011a}{{131}{}{{}}{{}}}
\bibcite{zou2005regularization}{{132}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\memsetcounter{lastsheet}{154}
\memsetcounter{lastpage}{146}
