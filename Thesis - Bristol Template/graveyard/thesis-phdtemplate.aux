\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@sortscheme{nty}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\select@language{english}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\par \penalty \@M \unhbox \voidb@x \hbox {}\hfill {\nag@@warning@vi  \bfseries  Page}\par \penalty \@M }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Tables}{iii}{section*.1}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\par \penalty \@M \textbf  {{\scshape  Table} \hfill Page}\par \penalty \@M }
\abx@aux@cite{Strategy2013}
\abx@aux@cite{Burbidge2007}
\abx@aux@cite{Tian}
\abx@aux@cite{Thompson2011}
\abx@aux@cite{DavenportSinglePixel}
\abx@aux@cite{Tibshirani1996}
\abx@aux@cite{Ji2008}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Figures}{v}{section*.2}}
\abx@aux@cite{mishali2010theory}
\abx@aux@cite{atia2}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\par \penalty \@M \textbf  {{\scshape  Figure} \hfill Page}\par \penalty \@M }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Classical Sensing}{5}{chapter.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.2.1}}
\abx@aux@cite{Candes2006}
\abx@aux@cite{donoho2}
\abx@aux@cite{Donoho}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A digram of current Spectral allocation \cite {Strategy2013}}}{6}{figure.2.1}}
\newlabel{spectrumalloc}{{\M@TitleReference {2.1}{A digram of current Spectral allocation \cite {Strategy2013}}}{6}{A digram of current Spectral allocation \cite {Strategy2013}}{figure.2.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}}{7}{figure.2.2}}
\newlabel{frequtil}{{\M@TitleReference {2.2}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}}{7}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}{figure.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Classical Sensing}{8}{section.2.2}}
\abx@aux@cite{yucek2009survey}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Narrowband Spectrum Sensing}{9}{section.2.3}}
\newlabel{h1}{{\M@TitleReference {2.3}{Narrowband Spectrum Sensing}}{9}{Narrowband Spectrum Sensing}{equation.2.3.3}{}}
\newlabel{h2}{{\M@TitleReference {2.3}{Narrowband Spectrum Sensing}}{9}{Narrowband Spectrum Sensing}{equation.2.3.4}{}}
\abx@aux@cite{xie2009optimal}
\abx@aux@cite{hamdi2010impact}
\abx@aux@cite{sahai2004some}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Energy Detection}{10}{section*.3}}
\abx@aux@cite{zhang2011adaptive}
\abx@aux@cite{olivieri2005scalable}
\abx@aux@cite{tandra2008snr}
\abx@aux@cite{oude2011lowering}
\abx@aux@cite{ye2007spectrum}
\abx@aux@cite{kim2007cyclostationary}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Cyclostationary Feature Detection}{11}{section*.4}}
\abx@aux@cite{Ghozzi2006}
\abx@aux@cite{lunden2007spectrum}
\abx@aux@cite{cabric2004implementation}
\abx@aux@cite{vcabric2005physical}
\newlabel{cyclic-covarience}{{\M@TitleReference {2.3}{Cyclostationary Feature Detection}}{12}{Cyclostationary Feature Detection}{equation.2.3.14}{}}
\newlabel{c1}{{\M@TitleReference {2.3}{Cyclostationary Feature Detection}}{12}{Cyclostationary Feature Detection}{equation.2.3.18}{}}
\newlabel{c2}{{\M@TitleReference {2.3}{Cyclostationary Feature Detection}}{12}{Cyclostationary Feature Detection}{equation.2.3.19}{}}
\abx@aux@cite{bhargavi2010performance}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Matched Filtering}{13}{section*.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Limitations}{13}{section*.6}}
\abx@aux@cite{Donoho2006}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Wideband Spectrum Sensing}{14}{section.2.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A digram of the Spectrum Sensing model \cite {Tian}}}{14}{figure.2.3}}
\newlabel{widebandspectra}{{\M@TitleReference {2.3}{A digram of the Spectrum Sensing model \cite {Tian}}}{14}{A digram of the Spectrum Sensing model \cite {Tian}}{figure.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Compressed Sensing}{14}{subsection.2.4.1}}
\abx@aux@cite{Duarte2008}
\abx@aux@cite{wakin2006architecture}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The operation of the single pixel camera at Rice University \cite {Thompson2011}, \cite {DavenportSinglePixel}}}{15}{figure.2.4}}
\newlabel{singlepixelcamera}{{\M@TitleReference {2.4}{The operation of the single pixel camera at Rice University \cite {Thompson2011}, \cite {DavenportSinglePixel}}}{15}{The operation of the single pixel camera at Rice University \cite {Thompson2011}, \cite {DavenportSinglePixel}}{figure.2.4}{}}
\abx@aux@cite{Chen1998}
\abx@aux@cite{Tropp2007}
\abx@aux@cite{Donoho2006a}
\abx@aux@cite{Emma}
\newlabel{minsamples}{{\M@TitleReference {2.4.1}{Compressed Sensing}}{16}{Compressed Sensing}{equation.2.4.24}{}}
\newlabel{program0}{{\M@TitleReference {2.4.1}{Compressed Sensing}}{16}{Compressed Sensing}{equation.2.4.25}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Incoherence, isometries and all that}{16}{section*.7}}
\abx@aux@cite{candes2011probabilistic}
\abx@aux@cite{foucart2013mathematical}
\abx@aux@cite{Candes2007}
\newlabel{RIP}{{\M@TitleReference {2.4.1}{Incoherence, isometries and all that}}{17}{Restricted Isometry Property}{equation.2.4.27}{}}
\newlabel{orthonormal}{{\M@TitleReference {2.4.1}{Incoherence, isometries and all that}}{17}{Incoherence, isometries and all that}{equation.2.4.28}{}}
\newlabel{mudef}{{\M@TitleReference {2.4.1}{Incoherence, isometries and all that}}{17}{Incoherence, isometries and all that}{equation.2.4.29}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Short, Fat matrices}{17}{section*.8}}
\abx@aux@cite{donoho2009message}
\abx@aux@cite{tropp2006relax}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{18}{figure.2.5}}
\newlabel{l1l2}{{\M@TitleReference {2.5}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{18}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Reconstruction Algorithms}{18}{subsection.2.4.2}}
\abx@aux@cite{Chen1998a}
\abx@aux@cite{candes2007dantzig}
\abx@aux@cite{tropp2007signal}
\abx@aux@cite{blumensath2009iterative}
\abx@aux@cite{Beck2009}
\abx@aux@cite{wen2013improved}
\abx@aux@cite{figueiredo2003algorithm}
\abx@aux@cite{Yedidia2011}
\abx@aux@cite{metzler2014denoising}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Bayesian Compressive Sensing}{19}{section*.9}}
\newlabel{sec:BayesianCS}{{\M@TitleReference {2.4.2}{Bayesian Compressive Sensing}}{19}{Bayesian Compressive Sensing}{section*.9}{}}
\newlabel{CSequation}{{\M@TitleReference {2.4.2}{Bayesian Compressive Sensing}}{19}{Bayesian Compressive Sensing}{equation.2.4.33}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{20}{figure.2.6}}
\newlabel{laplacenormal}{{\M@TitleReference {2.6}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{20}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.2.6}{}}
\abx@aux@cite{Baron2010}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{21}{figure.2.7}}
\newlabel{bayesiancs}{{\M@TitleReference {2.7}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{21}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.2.7}{}}
\abx@aux@cite{watkincandes}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Compressive Sensing}{23}{chapter.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction and Preliminaries}{23}{section.3.1}}
\newlabel{sec:intro}{{\M@TitleReference {3.1}{Introduction and Preliminaries}}{23}{Introduction and Preliminaries}{section.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}RIP and Stable Embeddings}{24}{subsection.3.1.1}}
\abx@aux@cite{shalev2014understanding}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{25}{figure.3.1}}
\newlabel{l1l2}{{\M@TitleReference {3.1}{A visualisation of the Compressive Sensing problem as an under-determined system}}{25}{A visualisation of the Compressive Sensing problem as an under-determined system}{figure.3.1}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.1.2}{RIP and Stable Embeddings}}{25}{RIP}{theorem.3.1.2}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.1.2}{RIP and Stable Embeddings}}{25}{RIP}{equation.3.1.6}{}}
\abx@aux@cite{davenport2010signal}
\newlabel{def:d-stable}{{\M@TitleReference {3.1.7}{RIP and Stable Embeddings}}{26}{\(\delta \)-stable embedding}{equation.3.1.8}{}}
\newlabel{minsamples}{{\M@TitleReference {3.1.1}{RIP and Stable Embeddings}}{26}{RIP and Stable Embeddings}{equation.3.1.9}{}}
\abx@aux@cite{baraniuk2008simple}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Random Matrix Constructions}{27}{subsection.3.1.2}}
\newlabel{sec:mtx-contruction}{{\M@TitleReference {3.1.2}{Random Matrix Constructions}}{27}{Random Matrix Constructions}{subsection.3.1.2}{}}
\newlabel{cond:norm-pres}{{\M@TitleReference {1}{Random Matrix Constructions}}{27}{Norm preservation}{condition.1}{}}
\newlabel{cond:sub-Gauss}{{\M@TitleReference {2}{Random Matrix Constructions}}{27}{sub-Gaussian}{condition.2}{}}
\newlabel{cond:sub-Gauss concetration}{{3.1.2.10}{27}{sub-Gaussian}{equation.3.1.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Wishart Matrices}{27}{subsection.3.1.3}}
\abx@aux@cite{levequeMatrices}
\abx@aux@cite{tibshirani1996regression}
\abx@aux@cite{hastie2005elements}
\newlabel{remark: exp AtA}{{\M@TitleReference {3.1.16}{Wishart Matrices}}{28}{}{theorem.3.1.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Reconstruction Algorithms}{28}{subsection.3.1.4}}
\newlabel{program:bp}{{3.1.4.16}{28}{Reconstruction Algorithms}{equation.3.1.16}{}}
\newlabel{program:lasso}{{3.1.4.17}{28}{Reconstruction Algorithms}{equation.3.1.17}{}}
\newlabel{soln:lasso}{{3.1.4.18}{29}{Reconstruction Algorithms}{equation.3.1.18}{}}
\newlabel{program:ridge}{{3.1.4.19}{29}{Reconstruction Algorithms}{equation.3.1.19}{}}
\newlabel{soln:ridge}{{3.1.4.20}{29}{Reconstruction Algorithms}{equation.3.1.20}{}}
\newlabel{program:ell0}{{3.1.4.21}{29}{Reconstruction Algorithms}{equation.3.1.21}{}}
\newlabel{soln:l0}{{3.1.4.22}{29}{Reconstruction Algorithms}{equation.3.1.22}{}}
\newlabel{program:enat}{{3.1.4.23}{29}{Reconstruction Algorithms}{equation.3.1.23}{}}
\abx@aux@cite{bickel2009simultaneous}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{30}{figure.3.2}}
\newlabel{fig:l1l2}{{\M@TitleReference {3.2}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{30}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.3.2}{}}
\newlabel{program:enat}{{3.1.4.24}{30}{Reconstruction Algorithms}{equation.3.1.24}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{31}{figure.3.3}}
\newlabel{alg:IST}{{\M@TitleReference {3.3}{The Iterative Soft Thresholding Algorithm}}{31}{The Iterative Soft Thresholding Algorithm}{figure.3.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The OMP recovery algorithm}}{32}{figure.3.4}}
\newlabel{alg:omp}{{\M@TitleReference {3.4}{The OMP recovery algorithm}}{32}{The OMP recovery algorithm}{figure.3.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The AMP recovery algorithm}}{32}{figure.3.5}}
\newlabel{alg:amp}{{\M@TitleReference {3.5}{The AMP recovery algorithm}}{32}{The AMP recovery algorithm}{figure.3.5}{}}
\newlabel{CSequation}{{\M@TitleReference {3.1.4}{Reconstruction Algorithms}}{32}{Reconstruction Algorithms}{equation.3.1.27}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{33}{figure.3.6}}
\newlabel{laplacenormal}{{\M@TitleReference {3.6}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{33}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.3.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{34}{figure.3.7}}
\newlabel{bayesiancs}{{\M@TitleReference {3.7}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{34}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.7}{}}
\abx@aux@cite{Zhang2011b}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Compressive Sensing Architechtures}{35}{section.3.2}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {3.2}{Compressive Sensing Architechtures}}{35}{Compressive Sensing Architechtures}{section.3.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Modulated Wideband Converter}{35}{subsection.3.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{35}{figure.3.8}}
\newlabel{msevssnr0}{{\M@TitleReference {3.8}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{35}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.3.8}{}}
\newlabel{system}{{3.2.1.36}{36}{Modulated Wideband Converter}{equation.3.2.36}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Random Demodulator}{36}{subsection.3.2.2}}
\abx@aux@cite{Dahlman2014}
\abx@aux@cite{akan2009cognitive}
\abx@aux@cite{stevenson2009ieee}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}ADMM}{39}{chapter.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{39}{section.4.1}}
\abx@aux@cite{Mishali2010a}
\abx@aux@cite{Mishali2009}
\abx@aux@cite{Mishali2011}
\abx@aux@cite{tropp2010beyond}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Wideband Spectrum Sensing}{40}{section.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A digram of the Spectrum Sensing model \cite {Tian}}}{40}{figure.4.1}}
\newlabel{widebandspectra}{{\M@TitleReference {4.1}{A digram of the Spectrum Sensing model \cite {Tian}}}{40}{A digram of the Spectrum Sensing model \cite {Tian}}{figure.4.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Compressed Sensing}{41}{subsection.4.2.1}}
\newlabel{minsamples}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{42}{Compressed Sensing}{equation.4.2.3}{}}
\newlabel{RIP}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{42}{Compressed Sensing}{equation.4.2.4}{}}
\newlabel{orthonormal}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{42}{Compressed Sensing}{equation.4.2.5}{}}
\newlabel{mudef}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{43}{Compressed Sensing}{equation.4.2.6}{}}
\newlabel{programl0}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{43}{Compressed Sensing}{equation.4.2.7}{}}
\newlabel{programl0}{{\M@TitleReference {4.2.1}{Compressed Sensing}}{43}{Compressed Sensing}{equation.4.2.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}RIPless Theory}{43}{subsection.4.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Short, Fat matrices}{43}{section*.10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{44}{figure.4.2}}
\newlabel{l1l2}{{\M@TitleReference {4.2}{A visualisation of the Compressive Sensing problem as an under-determined system}}{44}{A visualisation of the Compressive Sensing problem as an under-determined system}{figure.4.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Bayesian Compressive Sensing}{44}{section*.11}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{45}{figure.4.3}}
\newlabel{l1l2}{{\M@TitleReference {4.3}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{45}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.4.3}{}}
\newlabel{CSequation}{{\M@TitleReference {4.2.2}{Bayesian Compressive Sensing}}{45}{Bayesian Compressive Sensing}{equation.4.2.11}{}}
\abx@aux@cite{Mishali2010}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{46}{figure.4.4}}
\newlabel{laplacenormal}{{\M@TitleReference {4.4}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{46}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.4.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Sub-Nyquist Sampling techniques}{46}{subsection.4.2.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{47}{figure.4.5}}
\newlabel{bayesiancs}{{\M@TitleReference {4.5}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{47}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.4.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Wideband Modulated Converter}{47}{section*.12}}
\abx@aux@cite{Aldrouobi}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The operation of the Modulated Wideband Converter \cite {mishali2010theory}}}{48}{figure.4.6}}
\newlabel{bayesiancs}{{\M@TitleReference {4.6}{The operation of the Modulated Wideband Converter \cite {mishali2010theory}}}{48}{The operation of the Modulated Wideband Converter \cite {mishali2010theory}}{figure.4.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results and Simulations}{48}{section.4.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Group Testing vs Compressive Sensing}}{49}{figure.4.7}}
\newlabel{GTvsCS}{{\M@TitleReference {4.7}{Group Testing vs Compressive Sensing}}{49}{Group Testing vs Compressive Sensing}{figure.4.7}{}}
\abx@aux@cite{mota2013d}
\abx@aux@cite{parikh2014proximal}
\abx@aux@cite{rockafellar1976monotone}
\abx@aux@cite{douglas1956numerical}
\abx@aux@cite{eckstein1992douglas}
\abx@aux@cite{Bristow2014}
\abx@aux@cite{heredia2015consensus}
\abx@aux@cite{sawatzky2014proximal}
\abx@aux@cite{o2013splitting}
\abx@aux@cite{Shi2013}
\abx@aux@cite{nishihara2015general}
\abx@aux@cite{ghadimi2015optimal}
\abx@aux@cite{goldstein2014fast}
\abx@aux@cite{chen2016direct}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Optimisation on Graphs}{51}{chapter.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}ADMM}{51}{section.5.1}}
\abx@aux@cite{Boyd2010a}
\abx@aux@cite{nesterov2005smooth}
\newlabel{LASSO}{{5.1.0.2}{52}{ADMM}{equation.5.1.2}{}}
\newlabel{LASSO-L0}{{5.1.0.3}{52}{ADMM}{equation.5.1.3}{}}
\newlabel{admm}{{5.1.0.4}{52}{ADMM}{equation.5.1.4}{}}
\newlabel{admm_form}{{5.1.0.5}{52}{ADMM}{equation.5.1.5}{}}
\newlabel{admm_algo}{{5.1.0.7}{52}{ADMM}{equation.5.1.7}{}}
\newlabel{eq:lasso-lagrangian}{{5.1.0.10}{53}{ADMM}{equation.5.1.10}{}}
\newlabel{admm_algo_lasso}{{5.1.0.13}{53}{ADMM}{equation.5.1.13}{}}
\newlabel{dellx}{{5.1.0.14}{53}{ADMM}{equation.5.1.14}{}}
\newlabel{optx}{{5.1.0.16}{53}{ADMM}{equation.5.1.16}{}}
\newlabel{dellz-positive}{{5.1.0.17}{54}{ADMM}{equation.5.1.17}{}}
\newlabel{dellz-negative}{{5.1.0.18}{54}{ADMM}{equation.5.1.18}{}}
\newlabel{zbounds}{{5.1.0.19}{54}{ADMM}{equation.5.1.19}{}}
\newlabel{optz}{{5.1.0.20}{54}{ADMM}{equation.5.1.20}{}}
\abx@aux@cite{moreau1965proximite}
\newlabel{hatx}{{5.1.0.23}{55}{ADMM}{equation.5.1.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}The Proximity Operator}{55}{subsection.5.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Properties}{56}{section*.13}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Motivation}{56}{section*.14}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Examples}{57}{section*.15}}
\newlabel{consensus}{{5.1.12}{58}{Consensus}{theorem.5.1.12}{}}
\newlabel{admm_consensus}{{5.1.12}{58}{Consensus}{theorem.5.1.12}{}}
\newlabel{consensus_iterations}{{5.1.1.56}{59}{Consensus}{equation.5.1.56}{}}
\newlabel{simple_consensus_iterations}{{5.1.1.58}{59}{Consensus}{equation.5.1.58}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Statistical Interpretation}{59}{subsection.5.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Acceleration}{60}{subsection.5.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Constrained Optimisation on Graphs}{60}{section.5.2}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {5.2}{Constrained Optimisation on Graphs}}{60}{Constrained Optimisation on Graphs}{section.5.2}{}}
\newlabel{constrainedbp}{{5.2.0.61}{60}{Constrained Optimisation on Graphs}{equation.5.2.61}{}}
\newlabel{constrainedbp}{{5.2.0.62}{60}{Constrained Optimisation on Graphs}{equation.5.2.62}{}}
\newlabel{barxc}{{5.2.0.63}{60}{}{equation.5.2.63}{}}
\newlabel{compact-constraints}{{5.2.0.64}{61}{Constrained Optimisation on Graphs}{equation.5.2.64}{}}
\newlabel{constrainedbp1}{{5.2.0.65}{61}{Constrained Optimisation on Graphs}{equation.5.2.65}{}}
\newlabel{aug-lagrange}{{5.2.0.66}{61}{Constrained Optimisation on Graphs}{equation.5.2.66}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of a network}}{62}{figure.5.1}}
\newlabel{efig:ex-network}{{\M@TitleReference {5.1}{An example of a network}}{62}{An example of a network}{figure.5.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{62}{figure.5.2}}
\newlabel{fig:incidence-matrix}{{\M@TitleReference {5.2}{The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{62}{The incidence matrix associated with Figure \eqref {efig:ex-network}}{figure.5.2}{}}
\newlabel{generic-iterations}{{5.2.0.70}{63}{Constrained Optimisation on Graphs}{equation.5.2.70}{}}
\newlabel{dadmm_algo_lasso}{{5.2.0.75}{64}{}{equation.5.2.75}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Joint Space-Frequency Model}{64}{section.5.3}}
\newlabel{basis_expansion}{{5.3.0.76}{64}{Joint Space-Frequency Model}{equation.5.3.76}{}}
\abx@aux@cite{bazerque2008}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}Results}{65}{section.5.4}}
\newlabel{sec:results}{{\M@TitleReference {5.4}{Results}}{65}{Results}{section.5.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{66}{figure.5.3}}
\newlabel{msevssnr0}{{\M@TitleReference {5.3}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{66}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.5.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{66}{figure.5.4}}
\newlabel{msevssnr1}{{\M@TitleReference {5.4}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{66}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}{figure.5.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{67}{figure.5.5}}
\newlabel{fig:differentLambda}{{\M@TitleReference {5.5}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{67}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}{figure.5.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{67}{figure.5.6}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.6}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{67}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusions}{67}{section.5.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{68}{figure.5.7}}
\newlabel{fig:spline_recon}{{\M@TitleReference {5.7}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{68}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{68}{figure.5.8}}
\newlabel{fig:steps_wavelets}{{\M@TitleReference {5.8}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{68}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{69}{figure.5.9}}
\newlabel{fig:wavelet_recon}{{\M@TitleReference {5.9}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{69}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{69}{figure.5.10}}
\newlabel{fig:wavelet_recon_no_pwer_2}{{\M@TitleReference {5.10}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{69}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{70}{figure.5.11}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.11}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{70}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{70}{figure.5.12}}
\newlabel{fig:steps_difference}{{\M@TitleReference {5.12}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{70}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.12}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{71}{figure.5.13}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.13}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{71}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{71}{figure.5.14}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.14}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{71}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.14}{}}
\abx@aux@cite{polo2009compressive}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Sensing with Heavyside Basis}{73}{chapter.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{73}{section.6.1}}
\abx@aux@cite{tian2006wavelet}
\abx@aux@cite{ling2015dlm}
\abx@aux@cite{mokhtari2015dqm}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2}Signal Model}{74}{section.6.2}}
\newlabel{basis}{{6.2.0.1}{74}{Signal Model}{equation.6.2.1}{}}
\newlabel{basis-expansion}{{6.2.0.4}{75}{Signal Model}{equation.6.2.4}{}}
\newlabel{def:a}{{\M@TitleReference {6.2.1}{Signal Model}}{75}{}{equation.6.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sensing Model}{75}{section.6.3}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {6.3}{Sensing Model}}{75}{Sensing Model}{section.6.3}{}}
\newlabel{dist_system}{{6.3.0.9}{76}{Sensing Model}{equation.6.3.9}{}}
\newlabel{system}{{6.3.0.10}{76}{Sensing Model}{equation.6.3.10}{}}
\newlabel{opt}{{6.3.0.11}{76}{Sensing Model}{equation.6.3.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}Constrained Optimisation on Graphs}{76}{section.6.4}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {6.4}{Constrained Optimisation on Graphs}}{76}{Constrained Optimisation on Graphs}{section.6.4}{}}
\newlabel{barxc}{{6.4.0.13}{76}{}{equation.6.4.13}{}}
\newlabel{constrainedbp}{{6.4.0.14}{77}{Constrained Optimisation on Graphs}{equation.6.4.14}{}}
\newlabel{compact-constraints}{{6.4.0.15}{77}{Constrained Optimisation on Graphs}{equation.6.4.15}{}}
\newlabel{constrainedbp1}{{6.4.0.16}{77}{Constrained Optimisation on Graphs}{equation.6.4.16}{}}
\newlabel{aug-lagrange}{{6.4.0.17}{77}{Constrained Optimisation on Graphs}{equation.6.4.17}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The algorithm at Node \(j\)}}{79}{figure.6.1}}
\newlabel{DADMM}{{\M@TitleReference {6.1}{The algorithm at Node \(j\)}}{79}{The algorithm at Node \(j\)}{figure.6.1}{}}
\newlabel{generic-iterations}{{6.4.0.21}{79}{Constrained Optimisation on Graphs}{equation.6.4.21}{}}
\newlabel{dadmm_algo_lasso}{{6.4.0.26}{80}{Constrained Optimisation on Graphs}{equation.6.4.26}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{80}{section.6.5}}
\newlabel{sec:results}{{\M@TitleReference {6.5}{Results}}{80}{Results}{section.6.5}{}}
\abx@aux@cite{shi2014linear}
\abx@aux@cite{su2014differential}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusions}{81}{section.6.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{82}{figure.6.2}}
\newlabel{different_sigs}{{\M@TitleReference {6.2}{Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{82}{Left to right: (a) The original signal. (b) The gradient \eqref {def:a} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }{figure.6.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{83}{figure.6.3}}
\newlabel{msevssnr0}{{\M@TitleReference {6.3}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{83}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands}{figure.6.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{83}{figure.6.4}}
\newlabel{fig:differentLambda}{{\M@TitleReference {6.4}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{83}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations}{figure.6.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Compressive Inference}{85}{chapter.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{85}{section.7.1}}
\abx@aux@cite{donoho2004neighborly}
\abx@aux@cite{davenport2007smashed}
\newlabel{minsamples}{{\M@TitleReference {7.1}{Introduction}}{86}{Introduction}{equation.7.1.7}{}}
\abx@aux@cite{schnelle2012compressive}
\abx@aux@cite{davenport2010wideband}
\abx@aux@cite{eftekhari2013matched}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.2}Preliminaries}{87}{section.7.2}}
\newlabel{sec:prelims}{{\M@TitleReference {7.2}{Preliminaries}}{87}{Preliminaries}{section.7.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}RIP and Stable Embeddings}{87}{subsection.7.2.1}}
\newlabel{def:RIP}{{\M@TitleReference {7.2.2}{RIP and Stable Embeddings}}{87}{RIP}{equation.7.2.11}{}}
\newlabel{def:d-stable}{{\M@TitleReference {7.2.5}{RIP and Stable Embeddings}}{88}{\(\delta \)-stable embedding}{equation.7.2.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Random Matrix Constructions}{88}{subsection.7.2.2}}
\newlabel{sec:mtx-contruction}{{\M@TitleReference {7.2.2}{Random Matrix Constructions}}{88}{Random Matrix Constructions}{subsection.7.2.2}{}}
\newlabel{cond:norm-pres}{{\M@TitleReference {3}{Random Matrix Constructions}}{88}{Norm preservation}{condition.3}{}}
\newlabel{cond:sub-Gauss}{{\M@TitleReference {4}{Random Matrix Constructions}}{88}{sub-Gaussian}{condition.4}{}}
\newlabel{cond:sub-Gauss concetration}{{7.2.2.13}{88}{sub-Gaussian}{equation.7.2.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Wishart Matrices}{88}{subsection.7.2.3}}
\newlabel{remark: exp AtA}{{\M@TitleReference {7.2.12}{Wishart Matrices}}{89}{}{theorem.7.2.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Maximum Likelihood estimation: non-compressive case}{89}{subsection.7.2.4}}
\newlabel{sec:max-like}{{\M@TitleReference {7.2.4}{Maximum Likelihood estimation: non-compressive case}}{89}{Maximum Likelihood estimation: non-compressive case}{subsection.7.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.3}Compressive Estimation}{91}{section.7.3}}
\newlabel{sec:estimation}{{\M@TitleReference {7.3}{Compressive Estimation}}{91}{Compressive Estimation}{section.7.3}{}}
\newlabel{log-like}{{7.3.0.31}{91}{Compressive Estimation}{equation.7.3.31}{}}
\newlabel{approx-log-like}{{7.3.0.35}{92}{Compressive Estimation}{equation.7.3.35}{}}
\newlabel{eq: compressive-estimator}{{7.3.0.36}{92}{Compressive Estimation}{equation.7.3.36}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Example: Single Spike}{93}{subsection.7.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces }}{93}{figure.7.1}}
\newlabel{fig:new_basis_25}{{\M@TitleReference {7.1}{}}{93}{}{figure.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Estimating a single rectangle}{93}{subsection.7.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces }}{94}{figure.7.2}}
\newlabel{fig:rectangle}{{\M@TitleReference {7.2}{}}{94}{}{figure.7.2}{}}
\newlabel{basis}{{7.3.2.42}{94}{Estimating a single rectangle}{equation.7.3.42}{}}
\newlabel{basis-expansion}{{7.3.2.43}{94}{Estimating a single rectangle}{equation.7.3.43}{}}
\newlabel{ss-estimator}{{7.3.2.51}{95}{Estimating a single rectangle}{equation.7.3.51}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces }}{95}{figure.7.3}}
\newlabel{fig:hhat}{{\M@TitleReference {7.3}{}}{95}{}{figure.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Estimating Frequency spectra}{95}{subsection.7.3.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{96}{figure.7.4}}
\newlabel{fig:hvb}{{\M@TitleReference {7.4}{Example of classification with OFCOM data, 35 changepoints}}{96}{Example of classification with OFCOM data, 35 changepoints}{figure.7.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{96}{figure.7.5}}
\newlabel{fig:hvb}{{\M@TitleReference {7.5}{Example of classification with OFCOM data, 55 changepoints}}{96}{Example of classification with OFCOM data, 55 changepoints}{figure.7.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces ROC for synthetic data, midly noisy}}{97}{figure.7.6}}
\newlabel{fig:hvb}{{\M@TitleReference {7.6}{ROC for synthetic data, midly noisy}}{97}{ROC for synthetic data, midly noisy}{figure.7.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces ROC for synthetic data, very noisy}}{97}{figure.7.7}}
\newlabel{fig:hvb}{{\M@TitleReference {7.7}{ROC for synthetic data, very noisy}}{97}{ROC for synthetic data, very noisy}{figure.7.7}{}}
\abx@aux@cite{Dorfman1943}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Group Testing}{99}{chapter.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction and notation}{99}{section.8.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Group Testing}{99}{subsection.8.1.1}}
\abx@aux@cite{du}
\abx@aux@cite{Hwang1972}
\abx@aux@cite{Aldridge2013}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}}{101}{figure.8.1}}
\newlabel{bayesiancs}{{\M@TitleReference {8.1}{The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}}{101}{The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}{figure.8.1}{}}
\abx@aux@cite{Chan2011}
\abx@aux@cite{Baldassini2013}
\newlabel{hwangbound}{{\M@TitleReference {8.1.1}{Bounds}}{102}{Bounds}{equation.8.1.9}{}}
\newlabel{compbound}{{\M@TitleReference {8.1.1}{Bounds}}{102}{Bounds}{equation.8.1.10}{}}
\abx@aux@cite{Sejdinovic2010}
\abx@aux@cite{Wadayama}
\abx@aux@cite{dorfman}
\abx@aux@cite{malyutov}
\abx@aux@cite{atia}
\abx@aux@cite{johnsonc8}
\abx@aux@cite{johnson33}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Comparison to Compressive Sensing}{103}{section*.19}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}The Probabilistic group testing problem}{103}{subsection.8.1.2}}
\abx@aux@cite{li5}
\abx@aux@cite{shental}
\abx@aux@cite{johnsonc10}
\abx@aux@cite{tan}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Group testing capacity}{104}{subsection.8.1.3}}
\newlabel{def:capacity}{{\M@TitleReference {8.1.1}{Group testing capacity}}{104}{}{theorem.8.1.1}{}}
\newlabel{eq:lower}{{8.1.3.11}{104}{}{equation.8.1.11}{}}
\newlabel{eq:upper}{{8.1.3.12}{104}{}{equation.8.1.12}{}}
\abx@aux@cite{hwang}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Main results}{105}{subsection.8.1.4}}
\newlabel{thm:mainold}{{\M@TitleReference {8.1.4}{Main results}}{105}{\cite {johnsonc10}}{theorem.8.1.4}{}}
\newlabel{eq:bja}{{8.1.4.13}{105}{Main results}{equation.8.1.13}{}}
\newlabel{cor:main}{{\M@TitleReference {8.1.5}{Main results}}{105}{}{theorem.8.1.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.2}Algorithms and existing results}{105}{section.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Upper bounds on success probability}{105}{subsection.8.2.1}}
\newlabel{sec:ub}{{\M@TitleReference {8.2.1}{Upper bounds on success probability}}{105}{Upper bounds on success probability}{subsection.8.2.1}{}}
\newlabel{thm:upper}{{\M@TitleReference {8.2.1}{Upper bounds on success probability}}{106}{}{theorem.8.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Binary search algorithms}{106}{subsection.8.2.2}}
\newlabel{thm:lower}{{\M@TitleReference {8.2.3}{Binary search algorithms}}{106}{}{theorem.8.2.3}{}}
\abx@aux@cite{aksoylar}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{107}{figure.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Summary of our contribution}{107}{subsection.8.2.3}}
\newlabel{sec:algo}{{\M@TitleReference {8.2.3}{Summary of our contribution}}{107}{Summary of our contribution}{subsection.8.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Wider context: sparse inference problems}{107}{subsection.8.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.3}Analysis and new bounds}{108}{section.8.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Searching a set of bounded ratio}{108}{subsection.8.3.1}}
\newlabel{sec:boundedratio}{{\M@TitleReference {8.3.1}{Searching a set of bounded ratio}}{108}{Searching a set of bounded ratio}{subsection.8.3.1}{}}
\newlabel{cond:ratio}{{\M@TitleReference {6}{Searching a set of bounded ratio}}{108}{Bounded Ratio Condition}{condition.6}{}}
\newlabel{eq:ratio}{{8.3.1.14}{108}{Bounded Ratio Condition}{equation.8.3.14}{}}
\newlabel{lem:sfstep}{{\M@TitleReference {8.3.1}{Searching a set of bounded ratio}}{108}{}{theorem.8.3.1}{}}
\newlabel{eq:depth}{{8.3.1.15}{108}{}{equation.8.3.15}{}}
\newlabel{eq:setbd}{{8.3.1.16}{108}{Searching a set of bounded ratio}{equation.8.3.16}{}}
\newlabel{eq:lengthbd}{{\M@TitleReference {8.3.1.17}{Searching a set of bounded ratio}}{108}{Searching a set of bounded ratio}{equation.8.3.17}{}}
\newlabel{rem:algo}{{\M@TitleReference {8.3.2}{Searching a set of bounded ratio}}{109}{}{theorem.8.3.2}{}}
\newlabel{lem:expset}{{\M@TitleReference {8.3.3}{Searching a set of bounded ratio}}{109}{}{theorem.8.3.3}{}}
\newlabel{eq:tbds}{{8.3.1.18}{109}{}{equation.8.3.18}{}}
\newlabel{eq:testsperdef}{{\M@TitleReference {8.3.1.19}{Searching a set of bounded ratio}}{109}{Searching a set of bounded ratio}{equation.8.3.19}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Discarding low probability items}{109}{subsection.8.3.2}}
\newlabel{sec:discard}{{\M@TitleReference {8.3.2}{Discarding low probability items}}{109}{Discarding low probability items}{subsection.8.3.2}{}}
\newlabel{lem:thresh}{{\M@TitleReference {8.3.4}{Discarding low probability items}}{109}{}{theorem.8.3.4}{}}
\newlabel{eq:thetadef}{{8.3.2.20}{109}{}{equation.8.3.20}{}}
\newlabel{eq:setpstar}{{8.3.2.21}{109}{}{equation.8.3.21}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Searching the entire set}{110}{subsection.8.3.3}}
\newlabel{eq:bincount}{{8.3.3.22}{110}{Searching the entire set}{equation.8.3.22}{}}
\newlabel{def:full}{{\M@TitleReference {8.3.5}{Searching the entire set}}{110}{}{theorem.8.3.5}{}}
\newlabel{prop:splitting}{{\M@TitleReference {8.3.6}{Searching the entire set}}{110}{}{theorem.8.3.6}{}}
\newlabel{it:count}{{\M@TitleReference {1}{Searching the entire set}}{110}{Searching the entire set}{Item.22}{}}
\abx@aux@cite{petrov}
\newlabel{eq:counting}{{8.3.3.23}{111}{Searching the entire set}{equation.8.3.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Bounding the expected number of tests}{111}{subsection.8.3.4}}
\newlabel{sec:expectation}{{\M@TitleReference {8.3.4}{Bounding the expected number of tests}}{111}{Bounding the expected number of tests}{subsection.8.3.4}{}}
\newlabel{prop:overall}{{\M@TitleReference {8.3.7}{Bounding the expected number of tests}}{111}{}{theorem.8.3.7}{}}
\newlabel{eq:tbd}{{8.3.4.24}{111}{}{equation.8.3.24}{}}
\newlabel{eq:total}{{\M@TitleReference {8.3.4.25}{Bounding the expected number of tests}}{111}{Bounding the expected number of tests}{equation.8.3.25}{}}
\newlabel{eq:toopt}{{\M@TitleReference {8.3.4.25}{Bounding the expected number of tests}}{111}{Bounding the expected number of tests}{equation.8.3.25}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Controlling the error probabilities}{112}{subsection.8.3.5}}
\newlabel{sec:main}{{\M@TitleReference {8.3.5}{Controlling the error probabilities}}{112}{Controlling the error probabilities}{subsection.8.3.5}{}}
\newlabel{thm:bernstein}{{\M@TitleReference {8.3.8}{Controlling the error probabilities}}{112}{Bernstein}{theorem.8.3.8}{}}
\newlabel{eq:bernstein}{{8.3.5.26}{112}{Bernstein}{equation.8.3.26}{}}
\newlabel{thm:main}{{\M@TitleReference {8.3.9}{Controlling the error probabilities}}{112}{}{theorem.8.3.9}{}}
\newlabel{eq:tnec}{{8.3.5.27}{112}{}{equation.8.3.27}{}}
\newlabel{it:part1}{{\M@TitleReference {1}{Controlling the error probabilities}}{112}{}{Item.25}{}}
\newlabel{eq:errorprob}{{8.3.5.28}{112}{}{equation.8.3.28}{}}
\newlabel{it:part2}{{\M@TitleReference {2}{Controlling the error probabilities}}{112}{}{Item.26}{}}
\newlabel{eq:vbd}{{8.3.5.29}{112}{Controlling the error probabilities}{equation.8.3.29}{}}
\newlabel{eq:ratio2}{{\M@TitleReference {8.3.5.30}{Controlling the error probabilities}}{113}{Controlling the error probabilities}{equation.8.3.30}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.4}Results}{113}{section.8.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{114}{figure.8.3}}
\newlabel{ubvslb}{{\M@TitleReference {8.3}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{114}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}{figure.8.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{114}{figure.8.4}}
\newlabel{testsvsalpha}{{\M@TitleReference {8.4}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{114}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }{figure.8.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.5}Discussion}{114}{section.8.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{115}{figure.8.5}}
\newlabel{testsvstheta}{{\M@TitleReference {8.5}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{115}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}{figure.8.5}{}}
\memsetcounter{lastsheet}{126}
\memsetcounter{lastpage}{118}
