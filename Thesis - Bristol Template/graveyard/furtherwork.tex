\section{Further Work and Project Plan}
Both the Compressive Sensing and Group Testing problems have been well studied for the case where there is no prior belief over where the non-zero components of the signal are (or equivalently, which items constitute the defective set). Despite some work being done solving the CS problem in the Bayesian paradigm, the prior distributions used are sparsifying (for example a Laplace prior), and not over the placement of the non-zero components. 

AS such, there is some 'low hanging fruit' to be picked: would using extra information about the likelihood of an item being defective improve the performance of recovery suitably defined? For example, would a Group Testing procedure require fewer tests if there was reason to believe that some subset of items was more likely to be defective. In the coin weighing example discussed above, the input set may contain a coin which is a different colour than the others  - by testing this 'suspicious' coin in the first weighing we could find the counterfeit in a single test (otherwise we would find the counterfeit in at most five tests - one for the suspicious coin, and four following the procedure outlined in the Group Testing section). 

This suggests that using prior information in could significantly reduce the number of tests required to recover the defective set, however if initial tests come back negative the performance of these algorithms could be worsened by using a non-iid distribution. 

To try and characterise this numerical simulations and probabilistic analyses are required. The goal is to design algorithms which have better average case performance than current GT algorithms by incorporating prior beliefs, whilst at the same time do no worse than current performance guarantees. In the example above we'd seek an algorithm that could find the counterfeit coin in a single weighing (by testing the suspicious coin initially), yet could guarantee to find the counterfeit in at most four weighings of the pan balance. 

For the 80 coins, 1 counterfeit discussed above this is probably not possible: adding an extra test which fails before proceeding with the standard approach will always be worse than simply assuming all coins are equally likely to be fake and proceeding accordingly. Do many cases like this exist, and what characterises them? Will testing suspicious items initially always give a hit in performance should those tests fail? 

A few directions into answering these questions are sketched below:

\begin{enumerate}
\item Implicit in the discussion of Hwang's algorithm above is that each item is defective with the same probability \( \frac{K}{N} \). Using prior knowledge over the input set we could test groups of size \(2^\beta\), where \(\beta\) is the empirical expectation of the input set. I.e. modify Hwang's algorithm to test groups in size roughly equal to the (empirical) expectation of the distribution over the input. \textbf{Timeline:} the rest of October 2013. \textbf{Risk: } this is a low risk activity and should take at most a fortnight - the Author has already implemented Hwang's algorithm, so all that needs to be done is to calculate the empiracle mean of the input distribution, and characterise the algorithm in a few cases.

\item Re-formulate the problem in terms of source coding as per \cite{Aldroubi} and represent the items in the input set as a Huffman tree. \textbf{Timeline:} October-December 2013. \textbf{Risk: } medium risk. The Group Testing algorithm will take place over the tree with th groups to test being the branches of the Huffman tree. A bit of the code is ready, however it's not clear how to fit it all together to realise a full solution

\item It has usually been assumed that defective items are independent. Is it possible to relax this assumption, and if so what are the consequences of this for Group Testing algorithms? For example, what happens if defectives occur in clumps? This is a high risk activity, and should take over 6 months to begin to characterise. \textbf{Timeline: }
October 2013-March 2014. \textbf{Risk: } medium risk. Defectives are currently assumed to be independent from each other in the classical Group Testing literature. Motivated by Spectrum Sensing for Cognitive Radios, where available frequencies are not necessarily independent between measurements, we will investigate Group Testing algorithms where the defectives are not independently distributed. Examples include inputs with defective which are pairwise independent, input sets in which the defectives are negatively dependent, and input sets where there is some (random or deterministic) process affecting the distribution of defectives.

\item Some work has been done considering Group Testing as the reverse of Coding \cite{Wadayama2013}, \cite{Sejdinovic2010}, but there is still much to be done. How close is the analogy? Coding theory has a wealth of noise cases - could these be imported into the Group Testing literature, and how useful will they be? \textbf{Timeline: } October 2013 - September 2014. \textbf{Risk: } medium to high risk. A few cases have been explicitly solved (the 'binary-symmetric' and 'erasure' models). Does this analogy extend further, and can techniques from Coding help us in Group Testing. Can we formulate a more general 'Shannon Theory' type theory for Group Testing?

\item To make these models suitable for engineering applications, noisy cases must be considered. \textbf{Timeline: } January - September 2013. \textbf{Risk: } High risk: whereas Group Testing has previously been utilised in Medium Access Control protocol design, it has not been applied to spectrum sensing as of yet. The exact mechanism that can robustly sense spectral opportunities, and make decisions needs careful consideration. Example problems include how should we sample spectra using Group Testing - will a series of wideband pulses over the TVWS band be sufficient, or are is there some other possibility? Once we have these samples, how do then infer spectral opportunities? The work of previous sections (non-iid input distributions, 2D input sets, defectives which are not independent in some way etc) will inform this part of the research. 

\item There currently exist no two-dimensional models of Group Testing in the literature. This is particularly relevant for the problem faced by Cognitive Radios: the radios could represent the measurement point on a geographic grid, and the 'groups' they need to infer are sets of available frequencies. \textbf{Timeline: }  January-April 2014. \textbf{Risk: } High risk: there currently is no work in the literature on 2D group testing. Conjectured methods of solving this problem include factor graph methods from Coding Theory. It's not clear that this is an appropriate method, however.
\end{enumerate}