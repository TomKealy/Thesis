\relax 
\bibstyle{plain}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Compressive Sensing}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction and Preliminaries}{19}}
\newlabel{sec:intro}{{2.1}{19}}
\citation{watkincandes}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}RIP and Stable Embeddings}{20}}
\citation{shalev2014understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{21}}
\newlabel{l1l2}{{2.1.1}{21}}
\newlabel{def:RIP}{{2.1.2}{21}}
\newlabel{def:RIP}{{2.1.2}{21}}
\newlabel{def:d-stable}{{2.1.7}{22}}
\citation{Candes2006}
\citation{davenport2010signal}
\citation{baraniuk2008simple}
\newlabel{minsamples}{{2.1.1}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random Matrix Constructions}{23}}
\newlabel{sec:mtx-contruction}{{2.1.2}{23}}
\newlabel{cond:norm-pres}{{1}{23}}
\newlabel{cond:sub-Gauss}{{2}{23}}
\citation{baraniuk2008simple}
\newlabel{cond:sub-Gauss concetration}{{2.1.2.10}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Wishart Matrices}{24}}
\citation{levequeMatrices}
\citation{Chen1998a}
\citation{tibshirani1996regression}
\newlabel{remark: exp AtA}{{2.1.16}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Reconstruction Algorithms}{25}}
\newlabel{program:bp}{{2.1.4.16}{25}}
\citation{hastie2005elements}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\newlabel{program:lasso}{{2.1.4.17}{26}}
\newlabel{soln:lasso}{{2.1.4.18}{26}}
\newlabel{program:ridge}{{2.1.4.19}{26}}
\newlabel{soln:ridge}{{2.1.4.20}{26}}
\newlabel{program:ell0}{{2.1.4.21}{26}}
\newlabel{soln:l0}{{2.1.4.22}{26}}
\citation{candes2007dantzig}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{27}}
\newlabel{fig:l1l2}{{2.1.2}{27}}
\newlabel{program:enat}{{2.1.4.23}{27}}
\newlabel{program:enat}{{2.1.4.24}{27}}
\citation{candes2007dantzig}
\citation{bickel2009simultaneous}
\citation{donoho2009message}
\citation{figueiredo2003algorithm}
\citation{tropp2007signal}
\citation{wen2013improved}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{29}}
\newlabel{alg:IST}{{2.1.3}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.4}{\ignorespaces The OMP recovery algorithm}}{29}}
\newlabel{alg:omp}{{2.1.4}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.5}{\ignorespaces The AMP recovery algorithm}}{30}}
\newlabel{alg:amp}{{2.1.5}{30}}
\newlabel{CSequation}{{2.1.4}{30}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Baron2010}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.6}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{31}}
\newlabel{laplacenormal}{{2.1.6}{31}}
\citation{Ji2008}
\citation{Yedidia2011}
\citation{metzler2014denoising}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.7}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{32}}
\newlabel{bayesiancs}{{2.1.7}{32}}
\citation{Zhang2011b}
\citation{Zhang2011b}
\citation{mishali2010theory}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Compressive Sensing Architechtures}{33}}
\newlabel{sec:sensingmodel}{{2.2}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Modulated Wideband Converter}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2.8}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{33}}
\newlabel{msevssnr0}{{2.2.8}{33}}
\newlabel{system}{{2.2.1.36}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Random Demodulator}{35}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Chapter 1}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{37}}
\citation{Strategy2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1.1}{\ignorespaces A digram of current Spectral allocation \cite  {Strategy2013}}}{38}}
\newlabel{spectrumalloc}{{3.1.1}{38}}
\citation{Burbidge2007}
\citation{Burbidge2007}
\citation{Candes2006}
\citation{donoho2}
\citation{Donoho}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{39}}
\newlabel{frequtil}{{3.1.2}{39}}
\citation{mishali2010theory}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Classical Sensing}{41}}
\citation{yucek2009survey}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Narrowband Spectrum Sensing}{42}}
\newlabel{h1}{{3.3}{42}}
\newlabel{h2}{{3.3}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Energy Detection}{43}}
\citation{xie2009optimal}
\citation{hamdi2010impact}
\citation{sahai2004some}
\citation{zhang2011adaptive}
\citation{olivieri2005scalable}
\citation{tandra2008snr}
\citation{oude2011lowering}
\citation{ye2007spectrum}
\citation{kim2007cyclostationary}
\@writefile{toc}{\contentsline {subsubsection}{Cyclostationary Feature Detection}{44}}
\newlabel{cyclic-covarience}{{3.3}{45}}
\citation{Ghozzi2006}
\citation{lunden2007spectrum}
\citation{cabric2004implementation}
\citation{vcabric2005physical}
\citation{Ghozzi2006}
\citation{cabric2004implementation}
\citation{yucek2009survey}
\citation{bhargavi2010performance}
\newlabel{c1}{{3.3}{46}}
\newlabel{c2}{{3.3}{46}}
\@writefile{toc}{\contentsline {subsubsection}{Matched Filtering}{46}}
\citation{Tian}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Wideband Spectrum Sensing}{47}}
\citation{Donoho2006}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.3}{\ignorespaces A digram of the Spectrum Sensing model \cite  {Tian}}}{48}}
\newlabel{widebandspectra}{{3.4.3}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Compressed Sensing}{48}}
\citation{Duarte2008}
\citation{wakin2006architecture}
\citation{Thompson2011}
\citation{DavenportSinglePixel}
\citation{Candes2006}
\citation{Chen1998}
\citation{Tropp2007}
\citation{Donoho2006a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.4}{\ignorespaces The operation of the single pixel camera at Rice University \cite  {Thompson2011}, \cite  {DavenportSinglePixel}}}{50}}
\newlabel{singlepixelcamera}{{3.4.4}{50}}
\newlabel{minsamples}{{3.4.1}{50}}
\newlabel{program0}{{3.4.1}{50}}
\citation{Emma}
\citation{candes2011probabilistic}
\citation{foucart2013mathematical}
\citation{Candes2007}
\@writefile{toc}{\contentsline {subsubsection}{Incoherence, isometries and all that}{51}}
\newlabel{RIP}{{3.4.1}{51}}
\newlabel{orthonormal}{{3.4.1}{51}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{donoho2009message}
\newlabel{mudef}{{3.4.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{Short, Fat matrices}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Reconstruction Algorithms}{52}}
\citation{tropp2006relax}
\citation{Chen1998a}
\citation{candes2007dantzig}
\citation{tropp2007signal}
\citation{blumensath2009iterative}
\citation{Beck2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.5}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{53}}
\newlabel{l1l2}{{3.4.5}{53}}
\citation{wen2013improved}
\citation{figueiredo2003algorithm}
\citation{Yedidia2011}
\citation{metzler2014denoising}
\@writefile{toc}{\contentsline {subsubsection}{Bayesian Compressive Sensing}{54}}
\newlabel{sec:BayesianCS}{{3.4.2}{54}}
\newlabel{CSequation}{{3.4.2}{54}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.6}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{55}}
\newlabel{laplacenormal}{{3.4.6}{55}}
\citation{Baron2010}
\citation{Ji2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.7}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{56}}
\newlabel{bayesiancs}{{3.4.7}{56}}
\citation{Dahlman2014}
\citation{akan2009cognitive}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Chapter 2}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{57}}
\citation{stevenson2009ieee}
\citation{Candes2006}
\citation{mishali2010theory}
\citation{Mishali2010a}
\citation{Mishali2009}
\citation{Mishali2011}
\citation{tropp2010beyond}
\citation{Tian}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Wideband Spectrum Sensing}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Compressed Sensing}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.1}{\ignorespaces A digram of the Spectrum Sensing model \cite  {Tian}}}{59}}
\newlabel{widebandspectra}{{4.2.1}{59}}
\citation{Candes2006}
\citation{Emma}
\citation{Candes2007}
\newlabel{minsamples}{{4.2.1}{61}}
\newlabel{RIP}{{4.2.1}{61}}
\newlabel{orthonormal}{{4.2.1}{61}}
\citation{Donoho2006a}
\citation{Chen1998}
\citation{Tropp2007}
\newlabel{mudef}{{4.2.1}{62}}
\newlabel{programl0}{{4.2.1}{62}}
\newlabel{programl0}{{4.2.1}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}RIPless Theory}{63}}
\@writefile{toc}{\contentsline {subsubsection}{Short, Fat matrices}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.2}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{63}}
\newlabel{l1l2}{{4.2.2}{63}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.3}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{64}}
\newlabel{l1l2}{{4.2.3}{64}}
\@writefile{toc}{\contentsline {subsubsection}{Bayesian Compressive Sensing}{64}}
\newlabel{CSequation}{{4.2.2}{64}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.4}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{65}}
\newlabel{laplacenormal}{{4.2.4}{65}}
\citation{Baron2010}
\citation{Ji2008}
\citation{Mishali2010}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.5}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{66}}
\newlabel{bayesiancs}{{4.2.5}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Sub-Nyquist Sampling techniques}{66}}
\citation{mishali2010theory}
\@writefile{toc}{\contentsline {subsubsection}{Wideband Modulated Converter}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.6}{\ignorespaces The operation of the Modulated Wideband Converter \cite  {mishali2010theory}}}{67}}
\newlabel{bayesiancs}{{4.2.6}{67}}
\citation{Aldrouobi}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results and Simulations}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3.7}{\ignorespaces Group Testing vs Compressive Sensing}}{69}}
\newlabel{GTvsCS}{{4.3.7}{69}}
\citation{Zhang2011b}
\citation{mota2013d}
\citation{mota2013d}
\citation{mota2013d}
\citation{parikh2014proximal}
\citation{rockafellar1976monotone}
\citation{douglas1956numerical}
\citation{eckstein1992douglas}
\citation{Bristow2014}
\citation{heredia2015consensus}
\citation{sawatzky2014proximal}
\citation{o2013splitting}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Chapter 3}{71}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}ADMM}{71}}
\citation{Shi2013}
\citation{nishihara2015general}
\citation{ghadimi2015optimal}
\citation{goldstein2014fast}
\citation{chen2016direct}
\citation{Boyd2010a}
\citation{nesterov2005smooth}
\newlabel{LASSO}{{5.1.0.2}{72}}
\newlabel{LASSO-L0}{{5.1.0.3}{72}}
\newlabel{admm}{{5.1.0.4}{72}}
\newlabel{admm_form}{{5.1.0.5}{73}}
\newlabel{admm_algo}{{5.1.0.7}{73}}
\newlabel{eq:lasso-lagrangian}{{5.1.0.10}{73}}
\newlabel{admm_algo_lasso}{{5.1.0.13}{73}}
\newlabel{dellx}{{5.1.0.14}{74}}
\newlabel{optx}{{5.1.0.16}{74}}
\newlabel{dellz-positive}{{5.1.0.17}{74}}
\newlabel{dellz-negative}{{5.1.0.18}{74}}
\newlabel{zbounds}{{5.1.0.19}{74}}
\newlabel{optz}{{5.1.0.20}{75}}
\newlabel{hatx}{{5.1.0.23}{75}}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}The Proximity Operator}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Properties}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{77}}
\@writefile{toc}{\contentsline {subsubsection}{Examples}{78}}
\newlabel{consensus}{{5.1.12}{79}}
\newlabel{admm_consensus}{{5.1.12}{79}}
\newlabel{consensus_iterations}{{5.1.1.56}{79}}
\newlabel{simple_consensus_iterations}{{5.1.1.58}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Statistical Interpretation}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Acceleration}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Constrained Optimisation on Graphs}{80}}
\newlabel{sec:opt-on-graphs}{{5.2}{80}}
\newlabel{constrainedbp}{{5.2.0.61}{81}}
\newlabel{constrainedbp}{{5.2.0.62}{81}}
\newlabel{barxc}{{5.2.0.63}{81}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{compact-constraints}{{5.2.0.64}{82}}
\newlabel{constrainedbp1}{{5.2.0.65}{82}}
\newlabel{aug-lagrange}{{5.2.0.66}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2.1}{\ignorespaces An example of a network}}{83}}
\newlabel{efig:ex-network}{{5.2.1}{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2.2}{\ignorespaces The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{83}}
\newlabel{fig:incidence-matrix}{{5.2.2}{83}}
\newlabel{generic-iterations}{{5.2.0.70}{85}}
\newlabel{dadmm_algo_lasso}{{5.2.0.75}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Joint Space-Frequency Model}{85}}
\newlabel{basis_expansion}{{5.3.0.76}{85}}
\citation{bazerque2008}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Results}{87}}
\newlabel{sec:results}{{5.4}{87}}
\citation{goldstein2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.3}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{88}}
\newlabel{msevssnr0}{{5.4.3}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusions}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.4}{\ignorespaces Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{89}}
\newlabel{msevssnr1}{{5.4.4}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.5}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{89}}
\newlabel{fig:differentLambda}{{5.4.5}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.6}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{90}}
\newlabel{fig:erroriterations}{{5.4.6}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.7}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{90}}
\newlabel{fig:spline_recon}{{5.4.7}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.8}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{91}}
\newlabel{fig:steps_wavelets}{{5.4.8}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.9}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{91}}
\newlabel{fig:wavelet_recon}{{5.4.9}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.10}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{92}}
\newlabel{fig:wavelet_recon_no_pwer_2}{{5.4.10}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.11}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{92}}
\newlabel{fig:erroriterations}{{5.4.11}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.12}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{93}}
\newlabel{fig:steps_difference}{{5.4.12}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.13}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{93}}
\newlabel{fig:steps_splines}{{5.4.13}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4.14}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{94}}
\newlabel{fig:steps_splines}{{5.4.14}{94}}
\citation{akan2009cognitive}
\citation{Candes2006}
\citation{mishali2010theory}
\citation{polo2009compressive}
\citation{tropp2010beyond}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Chapter 4 }{95}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{95}}
\citation{Zhang2011b}
\citation{tian2006wavelet}
\citation{tian2006wavelet}
\citation{ling2015dlm}
\citation{mokhtari2015dqm}
\citation{mota2013d}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Signal Model}{97}}
\newlabel{basis}{{6.2.0.1}{97}}
\newlabel{basis-expansion}{{6.2.0.4}{97}}
\newlabel{def:a}{{6.2.1}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sensing Model}{98}}
\newlabel{sec:sensingmodel}{{6.3}{98}}
\newlabel{dist_system}{{6.3.0.9}{98}}
\newlabel{system}{{6.3.0.10}{98}}
\newlabel{opt}{{6.3.0.11}{98}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Constrained Optimisation on Graphs}{98}}
\newlabel{sec:opt-on-graphs}{{6.4}{98}}
\newlabel{barxc}{{6.4.0.13}{99}}
\newlabel{constrainedbp}{{6.4.0.14}{99}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{compact-constraints}{{6.4.0.15}{100}}
\newlabel{constrainedbp1}{{6.4.0.16}{100}}
\newlabel{aug-lagrange}{{6.4.0.17}{100}}
\newlabel{generic-iterations}{{6.4.0.21}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4.1}{\ignorespaces The algorithm at Node \(j\)}}{103}}
\newlabel{DADMM}{{6.4.1}{103}}
\newlabel{dadmm_algo_lasso}{{6.4.0.26}{103}}
\citation{Chen1998}
\citation{bazerque2008}
\citation{bazerque2008}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{104}}
\newlabel{sec:results}{{6.5}{104}}
\citation{shi2014linear}
\citation{nishihara2015general}
\citation{su2014differential}
\citation{mokhtari2015dqm}
\citation{ling2015dlm}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5.2}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{105}}
\newlabel{different_sigs}{{6.5.2}{105}}
\citation{goldstein2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5.3}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{106}}
\newlabel{msevssnr0}{{6.5.3}{106}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusions}{106}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5.4}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{107}}
\newlabel{fig:differentLambda}{{6.5.4}{107}}
\citation{Candes2006}
\citation{Donoho2006}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Chapter 5}{109}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{109}}
\citation{Candes2006}
\citation{donoho2004neighborly}
\newlabel{minsamples}{{7.1}{110}}
\citation{davenport2010signal}
\citation{davenport2007smashed}
\citation{schnelle2012compressive}
\citation{davenport2010wideband}
\citation{eftekhari2013matched}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Preliminaries}{111}}
\newlabel{sec:prelims}{{7.2}{111}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}RIP and Stable Embeddings}{111}}
\citation{davenport2010signal}
\newlabel{def:RIP}{{7.2.2}{112}}
\newlabel{def:d-stable}{{7.2.5}{112}}
\citation{baraniuk2008simple}
\citation{baraniuk2008simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Random Matrix Constructions}{113}}
\newlabel{sec:mtx-contruction}{{7.2.2}{113}}
\newlabel{cond:norm-pres}{{3}{113}}
\newlabel{cond:sub-Gauss}{{4}{113}}
\newlabel{cond:sub-Gauss concetration}{{7.2.2.13}{113}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Wishart Matrices}{113}}
\citation{levequeMatrices}
\newlabel{remark: exp AtA}{{7.2.12}{114}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Maximum Likelihood estimation: non-compressive case}{114}}
\newlabel{sec:max-like}{{7.2.4}{114}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Compressive Estimation}{116}}
\newlabel{sec:estimation}{{7.3}{116}}
\newlabel{log-like}{{7.3.0.31}{116}}
\newlabel{approx-log-like}{{7.3.0.35}{117}}
\newlabel{eq: compressive-estimator}{{7.3.0.36}{117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Example: Single Spike}{118}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Estimating a single rectangle}{118}}
\newlabel{basis}{{7.3.2.42}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.1}{\ignorespaces }}{119}}
\newlabel{fig:new_basis_25}{{7.3.1}{119}}
\newlabel{basis-expansion}{{7.3.2.43}{119}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.2}{\ignorespaces }}{120}}
\newlabel{fig:rectangle}{{7.3.2}{120}}
\newlabel{ss-estimator}{{7.3.2.51}{120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Estimating Frequency spectra}{120}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.3}{\ignorespaces }}{121}}
\newlabel{fig:hhat}{{7.3.3}{121}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.4}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{121}}
\newlabel{fig:hvb}{{7.3.4}{121}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.5}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{122}}
\newlabel{fig:hvb}{{7.3.5}{122}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.6}{\ignorespaces ROC for synthetic data, midly noisy}}{122}}
\newlabel{fig:hvb}{{7.3.6}{122}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3.7}{\ignorespaces ROC for synthetic data, very noisy}}{123}}
\newlabel{fig:hvb}{{7.3.7}{123}}
\citation{Dorfman1943}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Chapter 6}{125}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction and notation}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Group Testing}{125}}
\citation{atia2}
\citation{du}
\citation{Hwang1972}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{127}}
\newlabel{bayesiancs}{{8.1.1}{127}}
\citation{Aldridge2013}
\citation{Chan2011}
\citation{Baldassini2013}
\citation{Emma}
\newlabel{hwangbound}{{8.1.1}{129}}
\newlabel{compbound}{{8.1.1}{129}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Compressive Sensing}{129}}
\citation{Sejdinovic2010}
\citation{Wadayama}
\citation{Baldassini2013}
\citation{dorfman}
\citation{dorfman}
\citation{du}
\citation{malyutov}
\citation{malyutov}
\citation{atia}
\citation{johnsonc8}
\citation{johnson33}
\citation{Wadayama}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}The Probabilistic group testing problem}{130}}
\citation{li5}
\citation{atia2}
\citation{dorfman}
\citation{shental}
\citation{atia}
\citation{johnsonc10}
\citation{johnson33}
\citation{tan}
\citation{atia}
\citation{tan}
\citation{johnsonc10}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Group testing capacity}{131}}
\newlabel{def:capacity}{{8.1.1}{131}}
\newlabel{eq:lower}{{8.1.3.11}{131}}
\newlabel{eq:upper}{{8.1.3.12}{131}}
\citation{johnsonc10}
\citation{li5}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{hwang}
\citation{du}
\citation{johnsonc10}
\citation{li5}
\citation{li5}
\citation{hwang}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Main results}{132}}
\newlabel{thm:mainold}{{8.1.4}{132}}
\newlabel{eq:bja}{{8.1.4.13}{132}}
\newlabel{cor:main}{{8.1.5}{132}}
\citation{li5}
\citation{johnsonc10}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Algorithms and existing results}{133}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Upper bounds on success probability}{133}}
\newlabel{sec:ub}{{8.2.1}{133}}
\newlabel{thm:upper}{{8.2.1}{133}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Binary search algorithms}{133}}
\citation{li5}
\citation{li5}
\citation{li5}
\citation{li5}
\citation{li5}
\newlabel{thm:lower}{{8.2.3}{134}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Summary of our contribution}{134}}
\newlabel{sec:algo}{{8.2.3}{134}}
\citation{aksoylar}
\citation{tan}
\citation{Candes2006}
\citation{donoho2}
\citation{Candes2006}
\citation{donoho2}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{135}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Wider context: sparse inference problems}{135}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Analysis and new bounds}{136}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Searching a set of bounded ratio}{136}}
\newlabel{sec:boundedratio}{{8.3.1}{136}}
\newlabel{cond:ratio}{{6}{136}}
\newlabel{eq:ratio}{{8.3.1.14}{136}}
\newlabel{lem:sfstep}{{8.3.1}{136}}
\newlabel{eq:depth}{{8.3.1.15}{136}}
\newlabel{eq:setbd}{{8.3.1.16}{136}}
\newlabel{eq:lengthbd}{{8.3.1.17}{136}}
\citation{li5}
\newlabel{rem:algo}{{8.3.2}{137}}
\newlabel{lem:expset}{{8.3.3}{137}}
\newlabel{eq:tbds}{{8.3.1.18}{137}}
\newlabel{eq:testsperdef}{{8.3.1.19}{137}}
\citation{li5}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Discarding low probability items}{138}}
\newlabel{sec:discard}{{8.3.2}{138}}
\newlabel{lem:thresh}{{8.3.4}{138}}
\newlabel{eq:thetadef}{{8.3.2.20}{138}}
\newlabel{eq:setpstar}{{8.3.2.21}{138}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Searching the entire set}{138}}
\newlabel{eq:bincount}{{8.3.3.22}{138}}
\newlabel{def:full}{{8.3.5}{139}}
\newlabel{prop:splitting}{{8.3.6}{139}}
\newlabel{it:count}{{1}{139}}
\newlabel{eq:counting}{{8.3.3.23}{139}}
\citation{petrov}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Bounding the expected number of tests}{140}}
\newlabel{sec:expectation}{{8.3.4}{140}}
\newlabel{prop:overall}{{8.3.7}{140}}
\newlabel{eq:tbd}{{8.3.4.24}{140}}
\newlabel{eq:total}{{8.3.4.25}{140}}
\newlabel{eq:toopt}{{8.3.4.25}{140}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Controlling the error probabilities}{140}}
\newlabel{sec:main}{{8.3.5}{140}}
\newlabel{thm:bernstein}{{8.3.8}{141}}
\newlabel{eq:bernstein}{{8.3.5.26}{141}}
\newlabel{thm:main}{{8.3.9}{141}}
\newlabel{eq:tnec}{{8.3.5.27}{141}}
\newlabel{it:part1}{{1}{141}}
\newlabel{eq:errorprob}{{8.3.5.28}{141}}
\newlabel{it:part2}{{2}{141}}
\newlabel{eq:vbd}{{8.3.5.29}{141}}
\newlabel{eq:ratio2}{{8.3.5.30}{142}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Results}{142}}
\citation{li5}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{143}}
\newlabel{ubvslb}{{8.4.3}{143}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{144}}
\newlabel{testsvsalpha}{{8.4.4}{144}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{144}}
\newlabel{testsvstheta}{{8.4.5}{144}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Discussion}{144}}
\bibdata{thesis}
\bibcite{akan2009cognitive}{1}
\bibcite{aksoylar}{2}
\bibcite{johnson33}{3}
\bibcite{Aldridge2013}{4}
\bibcite{atia2}{5}
\bibcite{atia}{6}
\bibcite{Baldassini2013}{7}
\bibcite{johnsonc10}{8}
\bibcite{baraniuk2008simple}{9}
\bibcite{Baron2010}{10}
\bibcite{bazerque2008}{11}
\bibcite{Beck2009}{12}
\bibcite{bhargavi2010performance}{13}
\bibcite{blumensath2009iterative}{14}
\bibcite{Boyd2010a}{15}
\bibcite{Bristow2014}{16}
\bibcite{Burbidge2007}{17}
\bibcite{vcabric2005physical}{18}
\bibcite{cabric2004implementation}{19}
\bibcite{Candes2007}{20}
\bibcite{Emma}{21}
\bibcite{candes2007dantzig}{22}
\bibcite{candes2011probabilistic}{23}
\bibcite{Candes2006}{24}
\bibcite{Chan2011}{25}
\bibcite{chen2016direct}{26}
\bibcite{Chen1998}{27}
\bibcite{Chen1998a}{28}
\bibcite{Dahlman2014}{29}
\bibcite{davenport2010signal}{30}
\bibcite{davenport2007smashed}{31}
\bibcite{davenport2010wideband}{32}
\bibcite{DavenportSinglePixel}{33}
\bibcite{Donoho2006}{34}
\bibcite{donoho2004neighborly}{35}
\bibcite{donoho2}{36}
\bibcite{Donoho2006a}{37}
\bibcite{donoho2009message}{38}
\bibcite{Donoho}{39}
\bibcite{dorfman}{40}
\bibcite{Dorfman1943}{41}
\bibcite{douglas1956numerical}{42}
\bibcite{du}{43}
\bibcite{Duarte2008}{44}
\bibcite{eckstein1992douglas}{45}
\bibcite{eftekhari2013matched}{46}
\bibcite{figueiredo2003algorithm}{47}
\bibcite{foucart2013mathematical}{48}
\bibcite{ghadimi2015optimal}{49}
\bibcite{Ghozzi2006}{50}
\bibcite{goldstein2014fast}{51}
\bibcite{hamdi2010impact}{52}
\bibcite{heredia2015consensus}{53}
\bibcite{Hwang1972}{54}
\bibcite{hwang}{55}
\bibcite{Ji2008}{56}
\bibcite{kim2007cyclostationary}{57}
\bibcite{levequeMatrices}{58}
\bibcite{li5}{59}
\bibcite{ling2015dlm}{60}
\bibcite{lunden2007spectrum}{61}
\bibcite{malyutov}{62}
\bibcite{metzler2014denoising}{63}
\bibcite{Mishali2010}{64}
\bibcite{Mishali2009}{65}
\bibcite{mishali2010theory}{66}
\bibcite{Mishali2010a}{67}
\bibcite{Mishali2011}{68}
\bibcite{mokhtari2015dqm}{69}
\bibcite{moreau1965proximite}{70}
\bibcite{mota2013d}{71}
\bibcite{nesterov2005smooth}{72}
\bibcite{Ukmobil}{73}
\bibcite{nishihara2015general}{74}
\bibcite{o2013splitting}{75}
\bibcite{olivieri2005scalable}{76}
\bibcite{oude2011lowering}{77}
\bibcite{parikh2014proximal}{78}
\bibcite{petrov}{79}
\bibcite{polo2009compressive}{80}
\bibcite{rockafellar1976monotone}{81}
\bibcite{sahai2004some}{82}
\bibcite{sawatzky2014proximal}{83}
\bibcite{schnelle2012compressive}{84}
\bibcite{Sejdinovic2010}{85}
\bibcite{johnsonc8}{86}
\bibcite{shental}{87}
\bibcite{shi2014linear}{88}
\bibcite{Shi2013}{89}
\bibcite{stevenson2009ieee}{90}
\bibcite{su2014differential}{91}
\bibcite{tan}{92}
\bibcite{tandra2008snr}{93}
\bibcite{Thompson2011}{94}
\bibcite{Tian}{95}
\bibcite{tian2006wavelet}{96}
\bibcite{Tibshirani1996}{97}
\bibcite{tropp2006relax}{98}
\bibcite{tropp2007signal}{99}
\bibcite{Tropp2007}{100}
\bibcite{tropp2010beyond}{101}
\bibcite{Wadayama}{102}
\bibcite{wakin2006architecture}{103}
\bibcite{wen2013improved}{104}
\bibcite{xie2009optimal}{105}
\bibcite{ye2007spectrum}{106}
\bibcite{Yedidia2011}{107}
\bibcite{yucek2009survey}{108}
\bibcite{Zhang2011b}{109}
\bibcite{zhang2011adaptive}{110}
