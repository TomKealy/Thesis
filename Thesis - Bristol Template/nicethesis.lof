\select@language {english}
\par \penalty \@M \textbf {{\scshape Figure} \hfill Page}\par \penalty \@M 
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces A digram of current Spectral allocation \cite {Strategy2013}}}{6}{figure.1.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}}{7}{figure.1.2}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{25}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}}{31}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{33}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The OMP recovery algorithm}}{33}{figure.3.4}
\contentsline {figure}{\numberline {3.5}{\ignorespaces An illustration of the orthognalisation step of OMP. \cite {blumensath2007difference}}}{34}{figure.3.5}
\contentsline {figure}{\numberline {3.6}{\ignorespaces The AMP recovery algorithm}}{35}{figure.3.6}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}}{36}{figure.3.7}
\contentsline {figure}{\numberline {3.8}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{36}{figure.3.8}
\contentsline {figure}{\numberline {3.9}{\ignorespaces }}{40}{figure.3.9}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{40}{figure.3.10}
\contentsline {figure}{\numberline {3.11}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}}{43}{figure.3.11}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of a network}}{70}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces The incidence matrix associated with Figure \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {efig:ex-network}\unskip \@@italiccorr )}}}}{70}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{76}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{76}{figure.5.4}
\contentsline {figure}{\numberline {5.5}{\ignorespaces The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations.}}{77}{figure.5.5}
\contentsline {figure}{\numberline {5.6}{\ignorespaces The progress of the distributed solver with \(\ell _1\) (blue) and \(ell_0\) (red) regularisation as a function of the number of iterations. \(\lambda _{\ell _1} = P\mathchoice {\setbox 0=\hbox {$\displaystyle \sqrt {2\qopname \relax o{log}{n}\tmspace +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\textstyle \sqrt {2\qopname \relax o{log}{n}\tmspace +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptstyle \sqrt {2\qopname \relax o{log}{n}\tmspace +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}{\setbox 0=\hbox {$\scriptscriptstyle \sqrt {2\qopname \relax o{log}{n}\tmspace +\thinmuskip {.1667em}}$}\dimen 0=\ht 0 \advance \dimen 0-0.2\ht 0 \setbox 2=\hbox {\vrule height\ht 0 depth -\dimen 0}{\box 0\lower 0.4pt\box 2}}\), \(\lambda _{\ell _0} = 1000\lambda _{\ell _1}\) }}{77}{figure.5.6}
\contentsline {figure}{\numberline {5.7}{\ignorespaces }}{78}{figure.5.7}
\contentsline {figure}{\numberline {5.8}{\ignorespaces }}{78}{figure.5.8}
\contentsline {figure}{\numberline {5.9}{\ignorespaces }}{79}{figure.5.9}
\contentsline {figure}{\numberline {5.10}{\ignorespaces }}{79}{figure.5.10}
\contentsline {figure}{\numberline {5.11}{\ignorespaces }}{80}{figure.5.11}
\addvspace {10pt}
\contentsline {figure}{\numberline {6.1}{\ignorespaces A single rectangle signal, simmilar to example \ref {ex:single-rect}. This signal is used as an illustrative demonstration for the method developed in this chapter.}}{87}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces An example of the estimate \(\mathaccentV {hat}002{h}\) for a single rectangle signal (in red) compared to the true cumulative vector \(h\) (in blue). Note how the estimate tracks the true signal, with little deviation.}}{88}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Synthetic signal used for experiments in this section}}{89}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in table \ref {alg:single-slot}), for a reconstruction of a the signal in \ref {ci-sig}. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{figure.6.4}
\contentsline {figure}{\numberline {6.5}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \({\math@bb {R}}^{1000}\), with noise added at an SNR of \(-4.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{90}{figure.6.5}
\contentsline {figure}{\numberline {6.6}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \({\math@bb {R}}^{1000}\), with noise added at an SNR of \(-10.5\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{figure.6.6}
\contentsline {figure}{\numberline {6.7}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in \ref {alg:single-slot}), for a signal in \({\math@bb {R}}^{1000}\), with noise added at an SNR of \(-18\)dB. The first number in the legend is the ratio \(m/n\), whilst the second is the area under the curve.}}{91}{figure.6.7}
\contentsline {figure}{\numberline {6.8}{\ignorespaces SNR vs AUC for different levels of undersampling (as indicated in the legend)}}{92}{figure.6.8}
\contentsline {figure}{\numberline {6.9}{\ignorespaces SNR vs AUC for different levels of undersampling (as indicated in the legend), this time zoomed out from figure \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {snrauc}\unskip \@@italiccorr )}}}}{92}{figure.6.9}
\contentsline {figure}{\numberline {6.10}{\ignorespaces ROC curves for the single-shot algorithm (as outlined in table ,l;/l\ref {alg:single-slot}), for a signal in \({\math@bb {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{93}{figure.6.10}
\contentsline {figure}{\numberline {6.11}{\ignorespaces ROC curves for the single-shot algorithm with correct changepoints, for a signal in \({\math@bb {R}}^{1000}\), with noise added at an SNR of \(-10\)dB. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve.}}{94}{figure.6.11}
\contentsline {figure}{\numberline {6.12}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{96}{figure.6.12}
\contentsline {figure}{\numberline {6.13}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{97}{figure.6.13}
\contentsline {figure}{\numberline {6.14}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{97}{figure.6.14}
\contentsline {figure}{\numberline {6.15}{\ignorespaces }}{99}{figure.6.15}
\contentsline {figure}{\numberline {6.16}{\ignorespaces }}{99}{figure.6.16}
\contentsline {figure}{\numberline {6.17}{\ignorespaces }}{100}{figure.6.17}
\contentsline {figure}{\numberline {6.18}{\ignorespaces }}{100}{figure.6.18}
\contentsline {figure}{\numberline {6.19}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{101}{figure.6.19}
\contentsline {figure}{\numberline {6.20}{\ignorespaces Example of classification with OFCOM data, 85 changepoints}}{101}{figure.6.20}
\contentsline {figure}{\numberline {6.21}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{102}{figure.6.21}
\contentsline {figure}{\numberline {6.22}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{102}{figure.6.22}
\addvspace {10pt}
\contentsline {figure}{\numberline {7.1}{\ignorespaces Example multiple time-slot signal}}{108}{figure.7.1}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Compressive Measurements of the previous signal \ref {gt}}}{108}{figure.7.2}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Period-change vector for the signal \ref {gt}}}{109}{figure.7.3}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Procedure for estimating occupancy of frequency spectra with multiple time slots}}{109}{figure.7.4}
\contentsline {figure}{\numberline {7.5}{\ignorespaces ROC curves for the multi-shot algorithm (as outlined in \ref {alg:multiple-slot}), for a signal in \({\math@bb {R}}^{300}\), over 5 time slots. The first number in the legend is the ratio \(n/m\), whilst the second is the area under the curve. }}{111}{figure.7.5}
\contentsline {figure}{\numberline {7.6}{\ignorespaces }}{112}{figure.7.6}
\contentsline {figure}{\numberline {7.7}{\ignorespaces }}{112}{figure.7.7}
\addvspace {10pt}
\contentsline {figure}{\numberline {8.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}}{115}{figure.8.1}
\contentsline {figure}{\numberline {8.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{122}{figure.8.2}
\contentsline {figure}{\numberline {8.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{130}{figure.8.3}
\contentsline {figure}{\numberline {8.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{130}{figure.8.4}
\contentsline {figure}{\numberline {8.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{131}{figure.8.5}
